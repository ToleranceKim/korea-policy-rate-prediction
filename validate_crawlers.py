#!/usr/bin/env python3
"""
í¬ë¡¤ëŸ¬ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
PostgreSQL ì—†ì´ë„ í¬ë¡¤ëŸ¬ êµ¬ì¡°ì™€ ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import sys
import os
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent
sys.path.append(str(project_root))

def validate_mpb_crawler():
    """MPB í¬ë¡¤ëŸ¬ ê²€ì¦"""
    print("ğŸ•·ï¸  MPB í¬ë¡¤ëŸ¬ ê²€ì¦ ì¤‘...")
    
    mpb_path = project_root / "crawler" / "MPB" / "mpb_crawler"
    spider_path = mpb_path / "spiders" / "mpb_crawler.py"
    
    if not mpb_path.exists():
        print("âŒ MPB í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤")
        return False
    
    if not spider_path.exists():
        print("âŒ MPB ìŠ¤íŒŒì´ë” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
        return False
    
    try:
        # Import check
        sys.path.append(str(mpb_path))
        from spiders.mpb_crawler import MpbCrawlerSpider
        spider = MpbCrawlerSpider()
        
        print(f"âœ… MPB í¬ë¡¤ëŸ¬ ë¡œë“œ ì„±ê³µ")
        print(f"   - ìŠ¤íŒŒì´ë”ëª…: {spider.name}")
        print(f"   - í—ˆìš© ë„ë©”ì¸: {spider.allowed_domains}")
        return True
        
    except Exception as e:
        print(f"âŒ MPB í¬ë¡¤ëŸ¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

def validate_news_crawlers():
    """ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ê²€ì¦"""
    print("\nğŸ“° ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ê²€ì¦ ì¤‘...")
    
    crawlers = [
        {
            'name': 'Yonhap News',
            'path': project_root / "crawler" / "yh" / "yh_crawler" / "yh_crawler",
            'spider_file': 'spiders/yh_spider.py',
            'spider_class': 'YhSpider'
        },
        {
            'name': 'Edaily',
            'path': project_root / "crawler" / "edaily" / "edaily_crawler" / "edaily_crawler",
            'spider_file': 'spiders/edaily_spider.py',
            'spider_class': 'EdailySpider'
        }
    ]
    
    success_count = 0
    
    for crawler in crawlers:
        spider_path = crawler['path'] / crawler['spider_file']
        
        if not crawler['path'].exists():
            print(f"âŒ {crawler['name']} í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤")
            continue
            
        if not spider_path.exists():
            print(f"âŒ {crawler['name']} ìŠ¤íŒŒì´ë” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            continue
        
        try:
            # Read spider file to check basic structure
            with open(spider_path, 'r', encoding='utf-8') as f:
                content = f.read()
                
            if crawler['spider_class'] in content and 'scrapy.Spider' in content:
                print(f"âœ… {crawler['name']} í¬ë¡¤ëŸ¬ êµ¬ì¡° í™•ì¸ë¨")
                success_count += 1
            else:
                print(f"âŒ {crawler['name']} í¬ë¡¤ëŸ¬ êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤")
                
        except Exception as e:
            print(f"âŒ {crawler['name']} í¬ë¡¤ëŸ¬ ê²€ì¦ ì‹¤íŒ¨: {e}")
    
    return success_count == len(crawlers)

def validate_bond_crawler():
    """ì±„ê¶Œ í¬ë¡¤ëŸ¬ ê²€ì¦"""
    print("\nğŸ’° ì±„ê¶Œ í¬ë¡¤ëŸ¬ ê²€ì¦ ì¤‘...")
    
    bond_script = project_root / "crawler" / "BOND" / "bond_crawling.py"
    
    if not bond_script.exists():
        print("âŒ ì±„ê¶Œ í¬ë¡¤ëŸ¬ ìŠ¤í¬ë¦½íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
        return False
    
    try:
        # Check if the file has the basic structure
        with open(bond_script, 'r', encoding='utf-8') as f:
            content = f.read()
        
        required_elements = [
            'requests', 'BeautifulSoup', 'process_report', 
            'finance.naver.com', 'ThreadPoolExecutor'
        ]
        
        missing = [elem for elem in required_elements if elem not in content]
        
        if missing:
            print(f"âŒ ì±„ê¶Œ í¬ë¡¤ëŸ¬ì— í•„ìš”í•œ ìš”ì†Œê°€ ì—†ìŠµë‹ˆë‹¤: {missing}")
            return False
        
        print("âœ… ì±„ê¶Œ í¬ë¡¤ëŸ¬ êµ¬ì¡° í™•ì¸ë¨")
        print("   - ë©€í‹°ìŠ¤ë ˆë”© êµ¬í˜„ë¨")
        print("   - ë„¤ì´ë²„ ê¸ˆìœµ í¬ë¡¤ë§ êµ¬í˜„ë¨")
        return True
        
    except Exception as e:
        print(f"âŒ ì±„ê¶Œ í¬ë¡¤ëŸ¬ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return False

def validate_rates_crawler():
    """ì½œê¸ˆë¦¬ í¬ë¡¤ëŸ¬ ê²€ì¦"""
    print("\nğŸ“ˆ ì½œê¸ˆë¦¬ í¬ë¡¤ëŸ¬ ê²€ì¦ ì¤‘...")
    
    rates_path = project_root / "crawler" / "call_ratings" / "call_ratings_crawler"
    spider_path = rates_path / "spiders" / "call_ratings.py"
    
    if not rates_path.exists():
        print("âŒ ì½œê¸ˆë¦¬ í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤")
        return False
    
    if not spider_path.exists():
        print("âŒ ì½œê¸ˆë¦¬ ìŠ¤íŒŒì´ë” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
        return False
    
    try:
        # Read spider file
        with open(spider_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        if 'CallRatingsSpider' in content and 'scrapy.Spider' in content:
            print("âœ… ì½œê¸ˆë¦¬ í¬ë¡¤ëŸ¬ êµ¬ì¡° í™•ì¸ë¨")
            print("   - Scrapy ìŠ¤íŒŒì´ë” êµ¬í˜„ë¨")
            return True
        else:
            print("âŒ ì½œê¸ˆë¦¬ í¬ë¡¤ëŸ¬ êµ¬ì¡°ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤")
            return False
            
    except Exception as e:
        print(f"âŒ ì½œê¸ˆë¦¬ í¬ë¡¤ëŸ¬ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return False

def validate_main_pipeline():
    """ë©”ì¸ íŒŒì´í”„ë¼ì¸ ê²€ì¦"""
    print("\nğŸš€ ë©”ì¸ íŒŒì´í”„ë¼ì¸ ê²€ì¦ ì¤‘...")
    
    try:
        from main_pipeline import MPBPipeline
        
        # Check if all required methods exist
        pipeline = MPBPipeline()
        required_methods = [
            'run_crawlers', '_run_mpb_crawler', '_run_news_crawlers',
            '_run_bond_crawler', '_run_rates_crawler', 'process_data',
            'analyze_ngrams', 'train_models', 'evaluate_models',
            'generate_predictions', 'run_full_pipeline'
        ]
        
        missing_methods = []
        for method in required_methods:
            if not hasattr(pipeline, method):
                missing_methods.append(method)
        
        if missing_methods:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ì— í•„ìš”í•œ ë©”ì†Œë“œê°€ ì—†ìŠµë‹ˆë‹¤: {missing_methods}")
            return False
        
        print("âœ… ë©”ì¸ íŒŒì´í”„ë¼ì¸ êµ¬ì¡° í™•ì¸ë¨")
        print("   - ëª¨ë“  í¬ë¡¤ëŸ¬ ë©”ì†Œë“œ êµ¬í˜„ë¨")
        print("   - ë°ì´í„° ì²˜ë¦¬ ë©”ì†Œë“œ êµ¬í˜„ë¨")
        print("   - ëª¨ë¸ í›ˆë ¨/í‰ê°€ ë©”ì†Œë“œ êµ¬í˜„ë¨")
        return True
        
    except Exception as e:
        print(f"âŒ ë©”ì¸ íŒŒì´í”„ë¼ì¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ ê²€ì¦ í•¨ìˆ˜"""
    print("ğŸ” MPB Stance Mining í¬ë¡¤ëŸ¬ ê²€ì¦ ì‹œì‘")
    print("=" * 50)
    
    results = []
    
    # ê° ì»´í¬ë„ŒíŠ¸ ê²€ì¦
    results.append(("MPB í¬ë¡¤ëŸ¬", validate_mpb_crawler()))
    results.append(("ë‰´ìŠ¤ í¬ë¡¤ëŸ¬", validate_news_crawlers()))
    results.append(("ì±„ê¶Œ í¬ë¡¤ëŸ¬", validate_bond_crawler()))
    results.append(("ì½œê¸ˆë¦¬ í¬ë¡¤ëŸ¬", validate_rates_crawler()))
    results.append(("ë©”ì¸ íŒŒì´í”„ë¼ì¸", validate_main_pipeline()))
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 50)
    print("ğŸ¯ ê²€ì¦ ê²°ê³¼ ìš”ì•½:")
    
    passed = 0
    total = len(results)
    
    for name, result in results:
        status = "âœ… í†µê³¼" if result else "âŒ ì‹¤íŒ¨"
        print(f"   {name}: {status}")
        if result:
            passed += 1
    
    print(f"\nì´ {passed}/{total} ê°œ ì»´í¬ë„ŒíŠ¸ í†µê³¼")
    
    if passed == total:
        print("\nğŸ‰ ëª¨ë“  í¬ë¡¤ëŸ¬ê°€ ì •ìƒì ìœ¼ë¡œ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("\në‹¤ìŒ ë‹¨ê³„:")
        print("1. PostgreSQL ì„¤ì¹˜ ë° ì„¤ì •")
        print("2. ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™”")
        print("3. í¬ë¡¤ëŸ¬ ì‹¤í–‰ ë° ë°ì´í„° ìˆ˜ì§‘")
        print("4. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ë¶„ì„")
    else:
        print(f"\nâš ï¸  {total - passed}ê°œ ì»´í¬ë„ŒíŠ¸ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
        print("ìœ„ ì˜¤ë¥˜ë¥¼ ìˆ˜ì •í•œ í›„ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)