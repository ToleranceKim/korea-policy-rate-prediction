#!/usr/bin/env python3
"""
ë‰´ìŠ¤ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
2014-2025ë…„ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„°ì˜ ì™„ì „ì„±ê³¼ í’ˆì§ˆì„ ê²€ì¦
"""

import json
import os
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import statistics

def validate_news_data(base_path="/Users/lord_jubin/Desktop/my_git/mpb-stance-mining"):
    """ë‰´ìŠ¤ ë°ì´í„° ê²€ì¦ ë©”ì¸ í•¨ìˆ˜"""
    
    data_path = Path(base_path) / "crawler" / "data" / "unified"
    
    # í†µê³„ ë³€ìˆ˜
    total_articles = 0
    articles_by_source = defaultdict(int)
    articles_by_year = defaultdict(int)
    articles_by_month = defaultdict(int)
    content_lengths = []
    missing_content = 0
    truncated_content = 0
    duplicate_urls = set()
    seen_urls = set()
    
    # ëª¨ë“  ë‰´ìŠ¤ íŒŒì¼ ì²˜ë¦¬
    news_files = sorted(data_path.glob("news_*.json"))
    print(f"ì´ {len(news_files)}ê°œ íŒŒì¼ ë°œê²¬\n")
    
    for file_path in news_files:
        # íŒŒì¼ëª…ì—ì„œ ì—°ë„ì™€ ì›” ì¶”ì¶œ
        filename = file_path.stem  # news_2014_01
        parts = filename.split('_')
        year = int(parts[1])
        month = int(parts[2])
        
        print(f"ì²˜ë¦¬ ì¤‘: {filename}...", end=" ")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            month_total = 0
            
            # ê° ì†ŒìŠ¤ë³„ ê¸°ì‚¬ ì²˜ë¦¬
            for source, articles in data.items():
                if not isinstance(articles, list):
                    continue
                    
                for article in articles:
                    total_articles += 1
                    month_total += 1
                    articles_by_source[source] += 1
                    articles_by_year[year] += 1
                    articles_by_month[f"{year}-{month:02d}"] += 1
                    
                    # ì½˜í…ì¸  ê²€ì¦
                    content = article.get('content', '')
                    if not content:
                        missing_content += 1
                    else:
                        content_len = len(content)
                        content_lengths.append(content_len)
                        
                        # 3000ìì—ì„œ ì˜ë ¸ëŠ”ì§€ í™•ì¸
                        if content_len == 3000:
                            truncated_content += 1
                    
                    # URL ì¤‘ë³µ í™•ì¸
                    url = article.get('url', '')
                    if url:
                        if url in seen_urls:
                            duplicate_urls.add(url)
                        seen_urls.add(url)
            
            print(f"{month_total}ê°œ ê¸°ì‚¬")
            
        except Exception as e:
            print(f"ì˜¤ë¥˜: {e}")
    
    # í†µê³„ ê³„ì‚°
    avg_length = statistics.mean(content_lengths) if content_lengths else 0
    median_length = statistics.median(content_lengths) if content_lengths else 0
    min_length = min(content_lengths) if content_lengths else 0
    max_length = max(content_lengths) if content_lengths else 0
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*60)
    print("ë‰´ìŠ¤ ë°ì´í„° ê²€ì¦ ê²°ê³¼")
    print("="*60)
    
    print(f"\nğŸ“Š ì „ì²´ í†µê³„:")
    print(f"  - ì´ ê¸°ì‚¬ ìˆ˜: {total_articles:,}ê°œ")
    print(f"  - íŒŒì¼ ìˆ˜: {len(news_files)}ê°œ")
    print(f"  - ê¸°ê°„: 2014-01 ~ 2025-12")
    
    print(f"\nğŸ“ˆ ì†ŒìŠ¤ë³„ ë¶„í¬:")
    for source, count in sorted(articles_by_source.items()):
        percentage = (count / total_articles * 100) if total_articles > 0 else 0
        print(f"  - {source}: {count:,}ê°œ ({percentage:.1f}%)")
    
    print(f"\nğŸ“… ì—°ë„ë³„ ë¶„í¬:")
    for year in sorted(articles_by_year.keys()):
        count = articles_by_year[year]
        print(f"  - {year}ë…„: {count:,}ê°œ")
    
    print(f"\nğŸ“ ì½˜í…ì¸  í’ˆì§ˆ:")
    print(f"  - í‰ê·  ê¸¸ì´: {avg_length:,.0f}ì")
    print(f"  - ì¤‘ê°„ê°’ ê¸¸ì´: {median_length:,.0f}ì")
    print(f"  - ìµœì†Œ ê¸¸ì´: {min_length:,}ì")
    print(f"  - ìµœëŒ€ ê¸¸ì´: {max_length:,}ì")
    print(f"  - ë¹ˆ ì½˜í…ì¸ : {missing_content}ê°œ ({missing_content/total_articles*100:.2f}%)")
    print(f"  - 3000ì ì ˆë‹¨ ì˜ì‹¬: {truncated_content}ê°œ ({truncated_content/total_articles*100:.2f}%)")
    
    print(f"\nğŸ” ë°ì´í„° ë¬´ê²°ì„±:")
    print(f"  - ì¤‘ë³µ URL: {len(duplicate_urls)}ê°œ")
    print(f"  - ê³ ìœ  URL: {len(seen_urls)}ê°œ")
    
    # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
    quality_score = 100
    if missing_content > 0:
        quality_score -= (missing_content / total_articles * 20)
    if truncated_content > 0:
        quality_score -= (truncated_content / total_articles * 10)
    if len(duplicate_urls) > 0:
        quality_score -= (len(duplicate_urls) / total_articles * 10)
    if avg_length < 1000:
        quality_score -= 10
    
    print(f"\nâœ… í’ˆì§ˆ ì ìˆ˜: {quality_score:.1f}/100")
    
    if quality_score >= 90:
        print("   â†’ ìš°ìˆ˜: ë°ì´í„° í’ˆì§ˆì´ ë§¤ìš° ì¢‹ìŠµë‹ˆë‹¤!")
    elif quality_score >= 80:
        print("   â†’ ì–‘í˜¸: ëŒ€ë¶€ë¶„ì˜ ë°ì´í„°ê°€ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    elif quality_score >= 70:
        print("   â†’ ë³´í†µ: ì¼ë¶€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    else:
        print("   â†’ ì£¼ì˜: ë°ì´í„° í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    
    # ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    validation_result = {
        "validation_date": datetime.now().isoformat(),
        "total_articles": total_articles,
        "files_processed": len(news_files),
        "articles_by_source": dict(articles_by_source),
        "articles_by_year": dict(articles_by_year),
        "content_stats": {
            "avg_length": avg_length,
            "median_length": median_length,
            "min_length": min_length,
            "max_length": max_length,
            "missing_content": missing_content,
            "truncated_suspects": truncated_content
        },
        "integrity": {
            "duplicate_urls": len(duplicate_urls),
            "unique_urls": len(seen_urls)
        },
        "quality_score": quality_score
    }
    
    output_path = Path(base_path) / "data" / "news_validation_report.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(validation_result, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ê²€ì¦ ë³´ê³ ì„œ ì €ì¥: {output_path}")
    
    return validation_result

if __name__ == "__main__":
    validate_news_data()