#!/usr/bin/env python3
"""
MPB í†µí™”ì •ì±… ìŠ¤íƒ ìŠ¤ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
ìˆ˜ì§‘ ê¸°ê°„: 2014.08 ~ 2025.08 (ì•½ 11ë…„ì¹˜)
"""

import os
import sys
import subprocess
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta

class StanceMiningCrawler:
    def __init__(self):
        self.start_date = "2014-08-11"  # í˜„ì¬ í”„ë¡œì íŠ¸ ì‹œì‘ì¼
        self.end_date = "2025-08-11"    # í™•ì¥ ì¢…ë£Œì¼  
        self.mpb_start = "2014-08-11"   # MPB íšŒì˜ë¡
        self.news_start = "2014-08-11"  # ë‰´ìŠ¤/ì±„ê¶Œ
        
        # ì˜ˆìƒ ìˆ˜ì§‘ ê±´ìˆ˜ (11ë…„ì¹˜)
        self.estimated_counts = {
            'mpb': 100,      # ì•½ 100ì°¨ë¡€ íšŒì˜ë¡ (ì—° 8-9íšŒ)
            'news': 350000,  # ì•½ 35ë§Œê±´ ë‰´ìŠ¤ (ì—° 3ë§Œê±´)
            'bond': 35000    # ì•½ 3.5ë§Œê±´ ì±„ê¶Œ ë³´ê³ ì„œ (ì—° 3ì²œê±´)
        }
        
        print(f"ğŸ“‹ MPB ìŠ¤íƒ ìŠ¤ ë¶„ì„ìš© ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        print(f"ğŸ“… ìˆ˜ì§‘ ê¸°ê°„: {self.start_date} ~ {self.end_date} (ì•½ 11ë…„)")
        print(f"ğŸ¯ ì˜ˆìƒ: MPB {self.estimated_counts['mpb']}ê±´, ë‰´ìŠ¤ {self.estimated_counts['news']:,}ê±´, ì±„ê¶Œ {self.estimated_counts['bond']:,}ê±´")
        print(f"ğŸ’¾ ìŠ¤í† ë¦¬ì§€ ì ˆì•½ì„ ìœ„í•´ ìµœê·¼ 11ë…„ ë°ì´í„°ë§Œ ìˆ˜ì§‘")

    def run_mpb_crawler(self):
        """MPB íšŒì˜ë¡ í¬ë¡¤ë§ (2005.05~2017.12)"""
        print("\nğŸ›ï¸ MPB íšŒì˜ë¡ í¬ë¡¤ë§ ì‹œì‘...")
        
        # MPB í¬ë¡¤ëŸ¬ ì‹¤í–‰ (Scrapy)
        os.chdir('crawler/MPB')
        try:
            result = subprocess.run([
                'scrapy', 'crawl', 'mpb_crawler', 
                '-s', 'FEEDS={"../../data/raw/mpb_minutes.json": {"format": "json"}}'
            ], capture_output=True, text=True, timeout=7200)  # 2ì‹œê°„ íƒ€ì„ì•„ì›ƒ
            
            if result.returncode == 0:
                print("âœ… MPB í¬ë¡¤ë§ ì™„ë£Œ")
            else:
                print(f"âŒ MPB í¬ë¡¤ë§ ì‹¤íŒ¨: {result.stderr}")
                
        except subprocess.TimeoutExpired:
            print("â° MPB í¬ë¡¤ë§ íƒ€ì„ì•„ì›ƒ (2ì‹œê°„ ì´ˆê³¼)")
        finally:
            os.chdir('../..')

    def run_news_crawlers(self):
        """ë‰´ìŠ¤ í¬ë¡¤ë§ (ì—°í•©ì¸í¬ë§¥ìŠ¤, ì´ë°ì¼ë¦¬, ì—°í•©ë‰´ìŠ¤)"""
        print("\nğŸ“° ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")
        
        # ì—°í•©ë‰´ìŠ¤ í¬ë¡¤ë§ (API ê¸°ë°˜)
        self._run_yonhap_crawler()
        
        # ì´ë°ì¼ë¦¬ í¬ë¡¤ë§ (6ê°œì›” ë¶„í• )
        self._run_edaily_crawler()
        
        # ì¸í¬ë§¥ìŠ¤ëŠ” í˜„ì¬ 1ì¼ì¹˜ë§Œ êµ¬í˜„ë˜ì–´ ìˆì–´ ìŠ¤í‚µ
        print("âš ï¸ ì¸í¬ë§¥ìŠ¤ í¬ë¡¤ëŸ¬ëŠ” ë³„ë„ êµ¬í˜„ í•„ìš”")

    def _run_yonhap_crawler(self):
        """ì—°í•©ë‰´ìŠ¤ í¬ë¡¤ë§"""
        print("ğŸ“» ì—°í•©ë‰´ìŠ¤ í¬ë¡¤ë§...")
        
        # ë‚ ì§œ ë²”ìœ„ ìƒì„± (ì›”ë³„ ë¶„í• )
        start = datetime.strptime(self.news_start, '%Y-%m-%d')
        end = datetime.strptime(self.end_date, '%Y-%m-%d')
        
        current = start
        while current <= end:
            month_end = min(current + relativedelta(months=1) - timedelta(days=1), end)
            print(f"  ğŸ“… ìˆ˜ì§‘ ì¤‘: {current.strftime('%Y-%m-%d')} ~ {month_end.strftime('%Y-%m-%d')}")
            current = month_end + timedelta(days=1)
        
        print("â„¹ï¸ ì—°í•©ë‰´ìŠ¤: ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì • í›„ ì‹¤í–‰ ê°€ëŠ¥")

    def _run_edaily_crawler(self):
        """ì´ë°ì¼ë¦¬ í¬ë¡¤ë§ (6ê°œì›” ë‹¨ìœ„)"""
        print("ğŸ“º ì´ë°ì¼ë¦¬ í¬ë¡¤ë§...")
        
        # 6ê°œì›” ë‹¨ìœ„ë¡œ ë¶„í• 
        start = datetime.strptime(self.news_start, '%Y-%m-%d')
        end = datetime.strptime(self.end_date, '%Y-%m-%d')
        
        periods = []
        current = start
        while current <= end:
            period_end = min(current + relativedelta(months=6) - timedelta(days=1), end)
            periods.append((current, period_end))
            current = period_end + timedelta(days=1)
        
        print(f"  ğŸ“Š ì´ {len(periods)}ê°œ ê¸°ê°„ìœ¼ë¡œ ë¶„í• ")
        for i, (p_start, p_end) in enumerate(periods, 1):
            print(f"  ğŸ“… ê¸°ê°„ {i}: {p_start.strftime('%Y-%m-%d')} ~ {p_end.strftime('%Y-%m-%d')}")
        
        print("â„¹ï¸ ì´ë°ì¼ë¦¬: 6ê°œì›” ë¶„í•  ë¡œì§ êµ¬í˜„ í›„ ì‹¤í–‰")

    def run_bond_crawler(self):
        """ì±„ê¶Œ ë³´ê³ ì„œ í¬ë¡¤ë§ (2005.01~2017.12)"""
        print("\nğŸ’° ì±„ê¶Œ ë³´ê³ ì„œ í¬ë¡¤ë§ ì‹œì‘...")
        
        # ì±„ê¶Œ í¬ë¡¤ëŸ¬ëŠ” ê¸°ê°„ì´ ë„ˆë¬´ ê¸¸ë©´ íƒ€ì„ì•„ì›ƒ ìœ„í—˜
        # 1ë…„ ë‹¨ìœ„ë¡œ ë¶„í•  ì‹¤í–‰ ê¶Œì¥
        start_year = 2005
        end_year = 2017
        
        for year in range(start_year, end_year + 1):
            print(f"  ğŸ“… {year}ë…„ ì±„ê¶Œ ë³´ê³ ì„œ ìˆ˜ì§‘...")
            # ì‹¤ì œ ì‹¤í–‰ì€ bond_crawling.py ìˆ˜ì • í›„
        
        print("â„¹ï¸ ì±„ê¶Œ: ë‚ ì§œ ë²”ìœ„ ìˆ˜ì • í›„ ì‹¤í–‰ ê°€ëŠ¥")

    def validate_data(self):
        """ìˆ˜ì§‘ëœ ë°ì´í„° ê²€ì¦"""
        print("\nğŸ” ë°ì´í„° ìˆ˜ì§‘ ê²°ê³¼ ê²€ì¦...")
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        data_files = [
            'data/raw/mpb_minutes.json',
            'data/raw/yonhap_news.csv',
            'data/raw/edaily_news.csv',
            'data/raw/bond_reports.csv'
        ]
        
        for file_path in data_files:
            if os.path.exists(file_path):
                size = os.path.getsize(file_path) / (1024*1024)  # MB
                print(f"  âœ… {file_path}: {size:.1f}MB")
            else:
                print(f"  âŒ {file_path}: íŒŒì¼ ì—†ìŒ")
        
        print("\nğŸ“Š ì˜ˆìƒ ìˆ˜ì§‘ë¥ :")
        print(f"  ğŸ›ï¸ MPB: ? / {self.estimated_counts['mpb']} (ì˜ˆìƒ)")
        print(f"  ğŸ“° ë‰´ìŠ¤: ? / {self.estimated_counts['news']:,} (ì˜ˆìƒ)")
        print(f"  ğŸ’° ì±„ê¶Œ: ? / {self.estimated_counts['bond']:,} (ì˜ˆìƒ)")

    def run(self):
        """ì „ì²´ í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("ğŸš€ MPB ìŠ¤íƒ ìŠ¤ ë¶„ì„ìš© ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘\n")
        
        # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs('data/raw', exist_ok=True)
        
        try:
            # 1ë‹¨ê³„: MPB íšŒì˜ë¡ (ê°€ì¥ ì¤‘ìš”)
            self.run_mpb_crawler()
            
            # 2ë‹¨ê³„: ë‰´ìŠ¤ ê¸°ì‚¬
            self.run_news_crawlers()
            
            # 3ë‹¨ê³„: ì±„ê¶Œ ë³´ê³ ì„œ
            self.run_bond_crawler()
            
            # 4ë‹¨ê³„: ë°ì´í„° ê²€ì¦
            self.validate_data()
            
            print("\nğŸ‰ ë°ì´í„° ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
            print("âš ï¸ ì¼ë¶€ í¬ë¡¤ëŸ¬ëŠ” ë‚ ì§œ ìˆ˜ì • í›„ ê°œë³„ ì‹¤í–‰ í•„ìš”")
            
        except KeyboardInterrupt:
            print("\nâ›” ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    crawler = StanceMiningCrawler()
    crawler.run() 