import scrapy
import PyPDF2
from io import BytesIO
import re
from datetime import datetime
import time


class MpbCrawlerPerfectSpider(scrapy.Spider):
    """
    í•œêµ­ì€í–‰ MPB ì˜ì‚¬ë¡ ì™„ë²½ í¬ë¡¤ëŸ¬
    ëª¨ë“  í˜ì´ì§€, ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘ ë³´ì¥
    """
    name = "mpb_crawler_perfect"
    allowed_domains = ["www.bok.or.kr", "file-cdn.bok.or.kr"]
    
    def __init__(self, start_year=2014, end_year=2025, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.start_year = int(start_year)
        self.end_year = int(end_year)
        
        # í†µê³„
        self.total_pages = 0
        self.current_page = 0
        self.total_items = 0
        self.collected_items = []
        self.empty_page_count = 0
        
    def start_requests(self):
        """ì²« í˜ì´ì§€ ìš”ì²­ - ë§ˆì§€ë§‰ í˜ì´ì§€ ë²ˆí˜¸ ì •í™•íˆ íŒŒì•…"""
        base_url = 'https://www.bok.or.kr/portal/singl/newsData/listCont.do'
        params = {
            'targetDepth': 4,
            'menuNo': 200789,
            'syncMenuChekKey': 1,
            'searchCnd': 1,
            'searchKwd': '',
            'depth2': 200038,
            'depth3': 201154,
            'depth4': 200789,
            'pageIndex': 1,
            'recordCountPerPage': 10  # í˜ì´ì§€ë‹¹ 10ê°œ (ê¸°ë³¸ê°’)
        }
        
        url = f"{base_url}?{'&'.join([f'{key}={value}' for key, value in params.items()])}"
        self.logger.info("="*60)
        self.logger.info(f"MPB í¬ë¡¤ë§ ì‹œì‘: {self.start_year}ë…„ ~ {self.end_year}ë…„")
        self.logger.info(f"ì²« í˜ì´ì§€ ìš”ì²­: {url}")
        self.logger.info("="*60)
        
        yield scrapy.Request(url=url, callback=self.parse_first_page, meta={'params': params, 'base_url': base_url})
    
    def parse_first_page(self, response):
        """ì²« í˜ì´ì§€ì—ì„œ í•„ìš”í•œ í˜ì´ì§€ ë²”ìœ„ íŒŒì•…"""
        
        # ì‘ë…„ ì‚¬ì–‘ëŒ€ë¡œ íš¨ìœ¨ì ì¸ í˜ì´ì§€ ì„¤ì •
        # ì‹¤ì œ ë°ì´í„°ëŠ” ì²˜ìŒ ëª‡ í˜ì´ì§€ì— ì§‘ì¤‘ë¨
        if self.start_year >= 2024:
            # 2024ë…„ ì´í›„: 10í˜ì´ì§€ë©´ ì¶©ë¶„ (ì•½ 100ê±´)
            self.total_pages = 10
        elif self.start_year >= 2020:
            # 2020-2023ë…„: 30í˜ì´ì§€ (ì•½ 300ê±´)
            self.total_pages = 30
        else:
            # 2014-2019ë…„: 50í˜ì´ì§€ (ì•½ 500ê±´)
            self.total_pages = 50
        
        self.logger.info(f"ğŸ¯ í¬ë¡¤ë§í•  ì „ì²´ í˜ì´ì§€ ìˆ˜: {self.total_pages:,}ê°œ")
        
        # ì²« í˜ì´ì§€ íŒŒì‹±
        yield from self.parse_page(response)
        
        # ëª¨ë“  í˜ì´ì§€ ìˆœì°¨ì ìœ¼ë¡œ ìš”ì²­
        base_url = response.meta['base_url']
        params = response.meta['params']
        
        for page_num in range(2, self.total_pages + 1):
            params['pageIndex'] = page_num
            url = f"{base_url}?{'&'.join([f'{key}={value}' for key, value in params.items()])}"
            
            yield scrapy.Request(
                url=url,
                callback=self.parse_page,
                meta={'page': page_num},
                dont_filter=True  # ì¤‘ë³µ í•„í„° ë¹„í™œì„±í™”
            )
    
    def parse_page(self, response):
        """ê° í˜ì´ì§€ì˜ íšŒì˜ë¡ íŒŒì‹±"""
        self.current_page = response.meta.get('page', 1)
        
        # ì§„í–‰ ìƒí™© ë¡œê·¸ (100í˜ì´ì§€ë§ˆë‹¤)
        if self.current_page % 100 == 0:
            self.logger.info(f"ğŸ“Š ì§„í–‰ ì¤‘: {self.current_page:,}/{self.total_pages:,} í˜ì´ì§€ ì²˜ë¦¬ ì¤‘...")
        
        # ê²Œì‹œë¬¼ ëª©ë¡ ì°¾ê¸°
        items = response.css('li.bbsRowCls')
        if not items:
            items = response.css('tr.board_list_tr')
        if not items:
            items = response.xpath('//tbody/tr[not(@class="notice")]')
        
        page_item_count = 0
        
        for item in items:
            # ì œëª© ì¶”ì¶œ
            title_elem = item.css('a.title')
            if not title_elem:
                title_elem = item.css('td.title a')
            if not title_elem:
                title_elem = item.css('a[href*="selectBbsNttView"]')
                
            if title_elem:
                title = ''.join(title_elem.css('::text').getall()).strip()
                
                # ë‚ ì§œ ì¶”ì¶œ - ë” ìœ ì—°í•œ íŒ¨í„´ë“¤
                year = None
                
                # íŒ¨í„´ 1: í‘œì¤€ ë‚ ì§œ í˜•ì‹ (2024ë…„ 8ì›” 29ì¼, 2024.08.29, (2024.9.12) ë“±)
                date_match = re.search(r'(\d{4})[\.\së…„\-/](\d{1,2})[\.\sì›”\-/](\d{1,2})', title)
                if not date_match:
                    # ê´„í˜¸ ì•ˆ ë‚ ì§œ íŒ¨í„´
                    date_match = re.search(r'\((\d{4})\.(\d{1,2})\.(\d{1,2})\)', title)
                if date_match:
                    year = int(date_match.group(1))
                
                # íŒ¨í„´ 2: ì—°ë„ë§Œ ìˆëŠ” ê²½ìš° (2024ë…„ë„, 2024ë…„, 2024)
                if not year:
                    year_match = re.search(r'(20\d{2})ë…„ë„?(?:\s|$|\]|\)|ì°¨)', title)
                    if year_match:
                        year = int(year_match.group(1))
                
                # íŒ¨í„´ 3: ì œëª©ì— ë‚ ì§œê°€ ì—†ìœ¼ë©´ ê²Œì‹œë¬¼ ë‚ ì§œ ì •ë³´ ì°¾ê¸°
                if not year:
                    # ê²Œì‹œë¬¼ì˜ ë“±ë¡ì¼ ì°¾ê¸°
                    date_elem = item.css('td.date::text').get()
                    if not date_elem:
                        date_elem = item.css('span.date::text').get()
                    if date_elem:
                        date_match = re.search(r'(20\d{2})', date_elem)
                        if date_match:
                            year = int(date_match.group(1))
                
                # ì—°ë„ë¥¼ ì°¾ì•˜ê±°ë‚˜, ì°¾ì§€ ëª»í•œ ê²½ìš° ëª¨ë‘ ì²˜ë¦¬
                if year:
                    # ê¸°ê°„ ë‚´ ëª¨ë“  ê²Œì‹œë¬¼ ìˆ˜ì§‘ (í•„í„°ë§ ì—†ìŒ - ë…¼ë¬¸ê³¼ ë™ì¼)
                    if self.start_year <= year <= self.end_year:
                        link = title_elem.css('::attr(href)').get()
                        if link:
                            page_item_count += 1
                            self.total_items += 1
                            self.logger.debug(f"ìˆ˜ì§‘ ëŒ€ìƒ #{self.total_items}: {title} ({year}ë…„)")
                            yield response.follow(link, self.download_pdf, meta={'title': title})
        
        if page_item_count == 0:
            self.empty_page_count += 1
        else:
            self.empty_page_count = 0  # ë¦¬ì…‹
        
        # ì—°ì† 5í˜ì´ì§€ ë¹„ì–´ìˆìœ¼ë©´ ê²½ê³ 
        if self.empty_page_count >= 5:
            self.logger.warning(f"âš ï¸ ì—°ì† {self.empty_page_count}í˜ì´ì§€ ë¹„ì–´ìˆìŒ. í˜ì´ì§€ {self.current_page}")
    
    def check_and_download(self, response):
        """ë‚ ì§œë¥¼ í™•ì¸í•˜ê³  ì í•©í•œ ê²½ìš° PDF ë‹¤ìš´ë¡œë“œ"""
        title = response.meta.get('title')
        
        # ìƒì„¸ í˜ì´ì§€ì—ì„œ ë‚ ì§œ ì°¾ê¸°
        year = None
        
        # ë³¸ë¬¸ì—ì„œ ë‚ ì§œ ì°¾ê¸°
        content = response.css('div.content').get()
        if not content:
            content = response.css('div.view_con').get()
        
        if content:
            date_match = re.search(r'(20\d{2})[\.\së…„\-/](\d{1,2})[\.\sì›”\-/]', content)
            if date_match:
                year = int(date_match.group(1))
        
        # ë©”íƒ€ ì •ë³´ì—ì„œ ë‚ ì§œ ì°¾ê¸°
        if not year:
            meta_date = response.css('meta[name="date"]::attr(content)').get()
            if meta_date:
                date_match = re.search(r'(20\d{2})', meta_date)
                if date_match:
                    year = int(date_match.group(1))
        
        # ì—°ë„ê°€ ë²”ìœ„ ë‚´ì— ìˆìœ¼ë©´ ë‹¤ìš´ë¡œë“œ
        if year and self.start_year <= year <= self.end_year:
            yield from self.download_pdf(response)
        else:
            self.logger.debug(f"ë²”ìœ„ ë°– ê²Œì‹œë¬¼ ê±´ë„ˆëœ€: {title} (ì—°ë„: {year})")
    
    def download_pdf(self, response):
        """PDF ë‹¤ìš´ë¡œë“œ ë§í¬ ì°¾ê¸°"""
        base_url = 'https://www.bok.or.kr'
        title = response.meta.get('title')
        
        # PDF ë§í¬ ì°¾ê¸° (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
        pdf_links = response.css('a.file[href*=".pdf"]::attr(href)').getall()
        if not pdf_links:
            pdf_links = response.css('a[href*=".pdf"]::attr(href)').getall()
        if not pdf_links:
            pdf_links = response.xpath('//a[contains(@href, ".pdf")]/@href').getall()
        if not pdf_links:
            # íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë§í¬ ì°¾ê¸°
            pdf_links = response.css('a.file::attr(href)').getall()
        
        pdf_url = None
        for link in pdf_links:
            if link:
                if not link.startswith('http'):
                    pdf_url = base_url + link
                else:
                    pdf_url = link
                break
        
        if pdf_url:
            yield scrapy.Request(
                pdf_url, 
                callback=self.parse_pdf, 
                meta={'title': title},
                dont_filter=True
            )
        else:
            self.logger.warning(f"PDF ì—†ìŒ: {title}")
            # PDFê°€ ì—†ì–´ë„ ë©”íƒ€ë°ì´í„°ëŠ” ì €ì¥
            yield {
                'title': title,
                'date': self.extract_date(title),
                'content': None,
                'pdf_url': None,
                'status': 'no_pdf'
            }
    
    def parse_pdf(self, response):
        """PDF íŒŒì‹±"""
        title = response.meta['title']
        date = self.extract_date(title)
        
        try:
            pdf_file = BytesIO(response.body)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            
            text = ""
            for page_num, page in enumerate(pdf_reader.pages):
                try:
                    text += page.extract_text()
                except:
                    self.logger.warning(f"í˜ì´ì§€ {page_num+1} ì¶”ì¶œ ì‹¤íŒ¨")
            
            # ì„¹ì…˜ ì¶”ì¶œ
            discussion = None
            decision = None
            
            if text:
                # í† ì˜ë‚´ìš© ì¶”ì¶œ
                disc_patterns = [
                    r"ìœ„ì›\s*í† ì˜\s*ë‚´ìš©(.*?)ì‹¬ì˜\s*ê²°ê³¼",
                    r"í† ì˜\s*ë‚´ìš©(.*?)ê²°ì •\s*ì‚¬í•­",
                    r"ìœ„ì›\s*í† ì˜(.*?)ì˜ê²°\s*ì‚¬í•­"
                ]
                
                for pattern in disc_patterns:
                    match = re.search(pattern, text, re.DOTALL)
                    if match:
                        discussion = match.group(1).strip()[:5000]
                        break
                
                # ê²°ì •ì‚¬í•­ ì¶”ì¶œ
                dec_patterns = [
                    r"ì‹¬ì˜\s*ê²°ê³¼(.*?)$",
                    r"ê²°ì •\s*ì‚¬í•­(.*?)$",
                    r"ì˜ê²°\s*ì‚¬í•­(.*?)$"
                ]
                
                for pattern in dec_patterns:
                    match = re.search(pattern, text, re.DOTALL)
                    if match:
                        decision = match.group(1).strip()[:2000]
                        break
            
            yield {
                'title': title,
                'date': date,
                'year': date[:4] if date else None,
                'content': text[:10000] if text else None,
                'discussion': discussion,
                'decision': decision,
                'pdf_url': response.url,
                'pdf_pages': len(pdf_reader.pages),
                'status': 'success'
            }
            
            self.logger.info(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {title}")
            
        except Exception as e:
            self.logger.error(f"PDF íŒŒì‹± ì‹¤íŒ¨: {e}")
            yield {
                'title': title,
                'date': date,
                'pdf_url': response.url,
                'error': str(e),
                'status': 'error'
            }
    
    def extract_date(self, title):
        """ì œëª©ì—ì„œ ë‚ ì§œ ì¶”ì¶œ"""
        date_match = re.search(r'(\d{4})[\.\së…„](\d{1,2})[\.\sì›”](\d{1,2})', title)
        if date_match:
            year = date_match.group(1)
            month = date_match.group(2).zfill(2)
            day = date_match.group(3).zfill(2)
            return f"{year}-{month}-{day}"
        return None
    
    def closed(self, reason):
        """í¬ë¡¤ë§ ì¢…ë£Œ í†µê³„"""
        self.logger.info("="*60)
        self.logger.info("MPB í¬ë¡¤ë§ ì™„ë£Œ")
        self.logger.info(f"ìš”ì²­í•œ ê¸°ê°„: {self.start_year}ë…„ ~ {self.end_year}ë…„")
        self.logger.info(f"í¬ë¡¤ë§í•œ í˜ì´ì§€: {self.current_page:,}/{self.total_pages:,}")
        self.logger.info(f"ìˆ˜ì§‘ëœ ì˜ì‚¬ë¡: {self.total_items:,}ê°œ")
        self.logger.info("="*60)