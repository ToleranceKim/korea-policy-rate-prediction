#!/usr/bin/env python3
"""
ì›”ë³„ ë¶„í•  ìˆ˜ì§‘ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (ê²½ë¡œ ìˆ˜ì •ë¨)
"""

import sys
import argparse
from pathlib import Path

# scripts/batch ë””ë ‰í„°ë¦¬ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent / "scripts" / "batch"))

from batch_collector import BatchCollector
from data_merger import DataMerger

def main():
    parser = argparse.ArgumentParser(description='ì›”ë³„ ë¶„í•  ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ')
    parser.add_argument('--crawlers', nargs='+', 
                        choices=['yonhap', 'edaily', 'infomax', 'bond', 'interest_rates', 'call_ratings', 'mpb'],
                        default=['yonhap', 'edaily', 'infomax'],
                        help='ìˆ˜ì§‘í•  í¬ë¡¤ëŸ¬ ì„ íƒ')
    parser.add_argument('--test', action='store_true', help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì†ŒëŸ‰ ìˆ˜ì§‘)')
    parser.add_argument('--resume', type=str, help='íŠ¹ì • ì›”ë¶€í„° ì¬ì‹œì‘ (YYYY-MM í˜•ì‹)')
    parser.add_argument('--merge-only', action='store_true', help='ìˆ˜ì§‘ ì—†ì´ ë³‘í•©ë§Œ ì‹¤í–‰')
    parser.add_argument('--status', action='store_true', help='í˜„ì¬ ìƒíƒœë§Œ í™•ì¸')
    parser.add_argument('--start-year', type=int, default=2015, help='ìˆ˜ì§‘ ì‹œì‘ ì—°ë„ (ê¸°ë³¸ê°’: 2015)')
    parser.add_argument('--end-year', type=int, default=2025, help='ìˆ˜ì§‘ ì¢…ë£Œ ì—°ë„ (ê¸°ë³¸ê°’: 2025)')
    parser.add_argument('--end-month', type=int, default=8, help='ì¢…ë£Œ ì—°ë„ì˜ ë§ˆì§€ë§‰ ì›” (ê¸°ë³¸ê°’: 8)')
    
    args = parser.parse_args()
    
    collector = BatchCollector()
    
    if args.status:
        collector.print_status()
        return
    
    if args.merge_only:
        print("ğŸ”„ Starting merge-only process...")
        merger = DataMerger()
        merger.merge_all_crawlers()
        return
    
    print(f"ğŸš€ Starting batch collection...")
    print(f"ğŸ“‹ Target crawlers: {args.crawlers}")
    
    if args.test:
        print("ğŸ§ª TEST MODE: Limited collection for validation")
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œëŠ” ìµœê·¼ 1ê°œì›”ë§Œ ìˆ˜ì§‘
        from datetime import datetime, timedelta
        end_date = datetime.now()
        start_date = end_date - timedelta(days=30)
        
        # í…ŒìŠ¤íŠ¸ìš© ìˆ˜ë™ ìˆ˜ì§‘
        for crawler_name in args.crawlers:
            if crawler_name in ['yonhap', 'edaily', 'infomax']:
                test_period = {
                    'key': 'test',
                    'start_date': start_date.strftime('%Y-%m-%d'),
                    'end_date': end_date.strftime('%Y-%m-%d')
                }
                config = collector.crawlers[crawler_name]
                collector.collect_monthly_news(crawler_name, config, test_period)
    else:
        # ì „ì²´ ì›”ë³„ ìˆ˜ì§‘
        collector.run_monthly_collection(args.crawlers, args.resume)
    
    # ìˆ˜ì§‘ ì™„ë£Œ í›„ ìƒíƒœ ì¶œë ¥
    print("\n" + "="*60)
    collector.print_status()
    
    # ë³‘í•© ì‹¤í–‰ ì—¬ë¶€ í™•ì¸
    try:
        if input("\nğŸ”„ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë³‘í•©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower() == 'y':
            merger = DataMerger()
            merger.merge_all_crawlers()
    except (EOFError, KeyboardInterrupt):
        print("\nSkipping merge step...")

if __name__ == "__main__":
    main()
