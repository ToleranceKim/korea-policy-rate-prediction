#!/usr/bin/env python3
"""
크롤러 누락 가능성 심층 분석 스크립트
각 크롤러의 헛점과 누락 가능성을 체계적으로 검증
"""

import requests
from bs4 import BeautifulSoup
import json
import re
import time
from datetime import datetime, timedelta
import sys

class CrawlerVulnerabilityAnalyzer:
    """크롤러 취약점 분석기"""
    
    def __init__(self):
        self.issues = []
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
    
    def analyze_mpb_crawler(self):
        """MPB 크롤러 취약점 분석"""
        print("\n" + "="*60)
        print("MPB 크롤러 심층 분석")
        print("="*60)
        
        vulnerabilities = []
        
        # 1. 날짜 형식 다양성 테스트
        print("\n1. 날짜 형식 취약점 검사...")
        test_dates = [
            "2024.08.29",  # 점 구분
            "2024년 8월 29일",  # 한글 + 한자리 월일
            "2024년08월29일",  # 띄어쓰기 없음
            "2024-08-29",  # 대시 구분
            "24.8.29",  # 짧은 연도
            "2024/08/29",  # 슬래시 구분
        ]
        
        # 실제 웹사이트에서 다양한 날짜 형식 확인
        for page in [1, 100, 500, 1000, 2000]:
            try:
                url = f"https://www.bok.or.kr/portal/singl/newsData/listCont.do?targetDepth=4&menuNo=200789&pageIndex={page}"
                resp = self.session.get(url)
                soup = BeautifulSoup(resp.text, 'html.parser')
                
                titles = soup.select('li.bbsRowCls a.title')
                for title in titles[:3]:  # 페이지당 3개만 확인
                    title_text = title.text.strip()
                    # 현재 정규식이 캐치 못하는 형식 찾기
                    if not re.search(r'(\d{4})[\.\s년](\d{1,2})[\.\s월](\d{1,2})', title_text):
                        if re.search(r'\d{4}', title_text):  # 연도는 있지만 패턴 매칭 실패
                            vulnerabilities.append({
                                'type': '날짜 형식 미인식',
                                'page': page,
                                'title': title_text,
                                'impact': 'HIGH - 데이터 누락'
                            })
            except Exception as e:
                print(f"  페이지 {page} 접근 실패: {e}")
        
        # 2. 페이지네이션 경계 테스트
        print("\n2. 페이지네이션 경계 취약점 검사...")
        
        # 마지막 페이지 근처 검사
        last_page = 3263
        for page in [last_page - 1, last_page, last_page + 1]:
            try:
                url = f"https://www.bok.or.kr/portal/singl/newsData/listCont.do?targetDepth=4&menuNo=200789&pageIndex={page}"
                resp = self.session.get(url)
                
                if page == last_page + 1:
                    # 페이지 초과 시 리다이렉트나 에러 확인
                    if resp.status_code == 200:
                        soup = BeautifulSoup(resp.text, 'html.parser')
                        items = soup.select('li.bbsRowCls')
                        if items:
                            vulnerabilities.append({
                                'type': '페이지 범위 초과 처리 미흡',
                                'page': page,
                                'detail': f'페이지 {page}에도 {len(items)}개 항목 존재',
                                'impact': 'MEDIUM - 무한 루프 가능성'
                            })
            except Exception as e:
                print(f"  페이지 {page} 테스트 실패: {e}")
        
        # 3. DOM 구조 변화 취약점
        print("\n3. DOM 구조 변화 취약점 검사...")
        
        # 여러 페이지에서 다양한 셀렉터 테스트
        selectors_to_test = [
            ('li.bbsRowCls', '기본 셀렉터'),
            ('tr.board_list_tr', '대체 셀렉터 1'),
            ('tbody tr:not(.notice)', '대체 셀렉터 2'),
            ('div.board_list li', '가능한 새 구조'),
            ('table.board_list tr', '테이블 기반 구조')
        ]
        
        for page in [1, 500, 1500, 2500]:
            try:
                url = f"https://www.bok.or.kr/portal/singl/newsData/listCont.do?targetDepth=4&menuNo=200789&pageIndex={page}"
                resp = self.session.get(url)
                soup = BeautifulSoup(resp.text, 'html.parser')
                
                working_selectors = []
                for selector, desc in selectors_to_test:
                    items = soup.select(selector)
                    if items:
                        working_selectors.append((selector, len(items)))
                
                if len(working_selectors) > 1:
                    vulnerabilities.append({
                        'type': 'DOM 구조 불일치',
                        'page': page,
                        'selectors': working_selectors,
                        'impact': 'HIGH - 일부 페이지 누락 가능'
                    })
            except Exception as e:
                print(f"  페이지 {page} DOM 검사 실패: {e}")
        
        # 4. JavaScript 동적 로딩 검사
        print("\n4. JavaScript 동적 로딩 취약점 검사...")
        
        try:
            url = "https://www.bok.or.kr/portal/singl/newsData/listCont.do?targetDepth=4&menuNo=200789&pageIndex=1"
            resp = self.session.get(url)
            
            # JavaScript 로딩 징후 확인
            if 'javascript:' in resp.text or 'ajax' in resp.text.lower():
                script_tags = re.findall(r'<script[^>]*>(.*?)</script>', resp.text, re.DOTALL)
                ajax_patterns = ['$.ajax', 'XMLHttpRequest', 'fetch(', 'axios.']
                
                for script in script_tags:
                    for pattern in ajax_patterns:
                        if pattern in script:
                            vulnerabilities.append({
                                'type': 'AJAX 동적 로딩',
                                'pattern': pattern,
                                'impact': 'CRITICAL - Scrapy가 놓칠 수 있음'
                            })
                            break
        except Exception as e:
            print(f"  JavaScript 검사 실패: {e}")
        
        # 5. 숨겨진 페이지/필터 파라미터 검사
        print("\n5. 숨겨진 파라미터 취약점 검사...")
        
        # 가능한 추가 파라미터 테스트
        test_params = [
            {'searchCnd': 1, 'searchKwd': '통화정책'},  # 검색어 필터
            {'recordCountPerPage': 20},  # 페이지당 개수 변경
            {'recordCountPerPage': 50},
            {'orderBy': 'date'},  # 정렬 방식
            {'categoryId': '1'},  # 카테고리 필터
        ]
        
        base_url = "https://www.bok.or.kr/portal/singl/newsData/listCont.do"
        base_params = {
            'targetDepth': 4,
            'menuNo': 200789,
            'pageIndex': 1
        }
        
        for test_param in test_params:
            try:
                params = {**base_params, **test_param}
                resp = self.session.get(base_url, params=params)
                soup = BeautifulSoup(resp.text, 'html.parser')
                items = soup.select('li.bbsRowCls')
                
                # 기본 요청과 비교
                resp_base = self.session.get(base_url, params=base_params)
                soup_base = BeautifulSoup(resp_base.text, 'html.parser')
                items_base = soup_base.select('li.bbsRowCls')
                
                if len(items) != len(items_base):
                    vulnerabilities.append({
                        'type': '숨겨진 파라미터 영향',
                        'param': test_param,
                        'base_count': len(items_base),
                        'test_count': len(items),
                        'impact': 'HIGH - 데이터 누락/중복 가능'
                    })
            except Exception as e:
                print(f"  파라미터 {test_param} 테스트 실패: {e}")
        
        # 6. PDF 링크 패턴 다양성
        print("\n6. PDF 링크 패턴 취약점 검사...")
        
        pdf_patterns = []
        for page in [10, 500, 1000, 2000]:
            try:
                # 상세 페이지 진입 필요
                list_url = f"https://www.bok.or.kr/portal/singl/newsData/listCont.do?targetDepth=4&menuNo=200789&pageIndex={page}"
                resp = self.session.get(list_url)
                soup = BeautifulSoup(resp.text, 'html.parser')
                
                first_link = soup.select_one('li.bbsRowCls a.title')
                if first_link and first_link.get('href'):
                    detail_url = 'https://www.bok.or.kr' + first_link['href']
                    detail_resp = self.session.get(detail_url)
                    detail_soup = BeautifulSoup(detail_resp.text, 'html.parser')
                    
                    # 다양한 PDF 링크 패턴 찾기
                    pdf_links = []
                    pdf_links.extend(detail_soup.select('a[href*=".pdf"]'))
                    pdf_links.extend(detail_soup.select('a.file'))
                    pdf_links.extend(detail_soup.select('a[onclick*="download"]'))
                    
                    for link in pdf_links:
                        href = link.get('href', '')
                        onclick = link.get('onclick', '')
                        pattern = f"href={href[:20]}... onclick={onclick[:20]}..."
                        if pattern not in pdf_patterns:
                            pdf_patterns.append(pattern)
            except Exception as e:
                print(f"  페이지 {page} PDF 패턴 검사 실패: {e}")
        
        if len(pdf_patterns) > 1:
            vulnerabilities.append({
                'type': 'PDF 링크 패턴 다양성',
                'patterns': pdf_patterns,
                'impact': 'MEDIUM - 일부 PDF 누락 가능'
            })
        
        return vulnerabilities
    
    def analyze_call_rates_crawler(self):
        """콜금리 크롤러 취약점 분석"""
        print("\n" + "="*60)
        print("콜금리 크롤러 심층 분석")
        print("="*60)
        
        vulnerabilities = []
        
        # 1. 날짜 범위 경계 테스트
        print("\n1. 날짜 범위 경계 취약점 검사...")
        
        # 경계 날짜들 테스트
        test_dates = [
            ('20131231', '20140101'),  # 시작 경계
            ('20251231', '20260101'),  # 종료 경계
            ('20200229', '20200301'),  # 윤년
            ('20190228', '20190301'),  # 평년
        ]
        
        for old_date, new_date in test_dates:
            try:
                url = f"https://www.korcham.net/nCham/Service/EconBrief/appl/ProspectBoardList.asp"
                params = {
                    'board_type': 1,
                    'daybt': 'OldNow',
                    'm_OldDate': old_date,
                    'm_NowDate': new_date,
                    'pageno': 1
                }
                resp = self.session.get(url, params=params)
                
                if resp.status_code != 200:
                    vulnerabilities.append({
                        'type': '날짜 경계 처리 오류',
                        'dates': f"{old_date} ~ {new_date}",
                        'status': resp.status_code,
                        'impact': 'HIGH - 경계 데이터 누락'
                    })
            except Exception as e:
                print(f"  날짜 {old_date}~{new_date} 테스트 실패: {e}")
        
        # 2. 테이블 구조 변화 검사
        print("\n2. 테이블 구조 변화 취약점 검사...")
        
        for page in [1, 50, 100, 192]:
            try:
                url = f"https://www.korcham.net/nCham/Service/EconBrief/appl/ProspectBoardList.asp"
                params = {
                    'board_type': 1,
                    'daybt': 'OldNow',
                    'm_OldDate': '20140101',
                    'm_NowDate': '20251231',
                    'pageno': page
                }
                resp = self.session.get(url, params=params)
                soup = BeautifulSoup(resp.text, 'html.parser')
                
                # 여러 테이블 구조 확인
                tables = soup.find_all('table')
                table_structures = []
                
                for table in tables:
                    rows = table.find_all('tr')
                    if rows:
                        cols = len(rows[0].find_all(['td', 'th']))
                        table_structures.append(cols)
                
                if len(set(table_structures)) > 1:
                    vulnerabilities.append({
                        'type': '테이블 구조 불일치',
                        'page': page,
                        'structures': table_structures,
                        'impact': 'MEDIUM - 데이터 파싱 오류 가능'
                    })
            except Exception as e:
                print(f"  페이지 {page} 테이블 검사 실패: {e}")
        
        # 3. 인코딩 문제 검사
        print("\n3. 인코딩 취약점 검사...")
        
        try:
            url = f"https://www.korcham.net/nCham/Service/EconBrief/appl/ProspectBoardList.asp"
            params = {'board_type': 1, 'pageno': 1}
            
            # 다양한 인코딩으로 시도
            encodings = ['utf-8', 'euc-kr', 'cp949', 'iso-8859-1']
            
            for encoding in encodings:
                resp = self.session.get(url, params=params)
                try:
                    text = resp.content.decode(encoding)
                    if '?' * 3 in text or '\\u' in text or '&#' in text:
                        vulnerabilities.append({
                            'type': '인코딩 문제',
                            'encoding': encoding,
                            'impact': 'LOW - 텍스트 깨짐'
                        })
                except:
                    pass
        except Exception as e:
            print(f"  인코딩 검사 실패: {e}")
        
        return vulnerabilities
    
    def analyze_interest_rates_crawler(self):
        """기준금리 크롤러 취약점 분석"""
        print("\n" + "="*60)
        print("기준금리 크롤러 심층 분석")
        print("="*60)
        
        vulnerabilities = []
        
        # 1. JavaScript 변수명 변경 가능성
        print("\n1. JavaScript 변수명 취약점 검사...")
        
        try:
            url = "https://www.bok.or.kr/portal/singl/baseRate/list.do?dataSeCd=01&menuNo=200643"
            resp = self.session.get(url)
            
            # 여러 가능한 변수명 패턴
            js_patterns = [
                r'chartObj2_s\s*=\s*(\[.*?\]);',
                r'chartData\s*=\s*(\[.*?\]);',
                r'rateData\s*=\s*(\[.*?\]);',
                r'var\s+data\s*=\s*(\[.*?\]);',
            ]
            
            found_patterns = []
            for pattern in js_patterns:
                if re.search(pattern, resp.text, re.DOTALL):
                    found_patterns.append(pattern)
            
            if len(found_patterns) == 0:
                vulnerabilities.append({
                    'type': 'JavaScript 변수 미발견',
                    'impact': 'CRITICAL - 전체 데이터 누락'
                })
            elif found_patterns[0] != r'chartObj2_s\s*=\s*(\[.*?\]);':
                vulnerabilities.append({
                    'type': 'JavaScript 변수명 변경됨',
                    'found': found_patterns[0],
                    'impact': 'HIGH - 코드 수정 필요'
                })
        except Exception as e:
            print(f"  JavaScript 검사 실패: {e}")
        
        # 2. 데이터 형식 변경 검사
        print("\n2. 데이터 형식 취약점 검사...")
        
        try:
            url = "https://www.bok.or.kr/portal/singl/baseRate/list.do?dataSeCd=01&menuNo=200643"
            resp = self.session.get(url)
            
            # 현재 형식: {date:"2014-08-14", rate:2.25}
            # 가능한 변형들
            format_patterns = [
                r'\{date:"[\d-]+",\s*rate:[\d.]+\}',  # 현재
                r'\{"date":"[\d-]+",\s*"rate":[\d.]+\}',  # JSON 표준
                r'\{d:"[\d-]+",\s*r:[\d.]+\}',  # 축약형
                r'\["[\d-]+",\s*[\d.]+\]',  # 배열형
            ]
            
            for pattern in format_patterns[1:]:
                if re.search(pattern, resp.text):
                    vulnerabilities.append({
                        'type': '데이터 형식 변경',
                        'pattern': pattern,
                        'impact': 'HIGH - 파싱 로직 수정 필요'
                    })
        except Exception as e:
            print(f"  형식 검사 실패: {e}")
        
        # 3. 폴백 메커니즘 검증
        print("\n3. 폴백 메커니즘 검사...")
        
        try:
            url = "https://www.bok.or.kr/portal/singl/baseRate/list.do?dataSeCd=01&menuNo=200643"
            resp = self.session.get(url)
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # HTML 테이블 존재 확인
            tables = soup.find_all('table')
            has_table_data = False
            
            for table in tables:
                rows = table.find_all('tr')
                if len(rows) > 1:  # 헤더 제외 데이터 존재
                    has_table_data = True
                    break
            
            # JavaScript 데이터 존재 확인
            has_js_data = bool(re.search(r'chartObj2_s\s*=\s*\[', resp.text))
            
            if has_table_data and not has_js_data:
                vulnerabilities.append({
                    'type': 'JavaScript 전용 의존',
                    'detail': 'HTML 테이블 있지만 사용 안함',
                    'impact': 'MEDIUM - 폴백 가능'
                })
            elif not has_table_data and not has_js_data:
                vulnerabilities.append({
                    'type': '데이터 소스 없음',
                    'impact': 'CRITICAL - 전체 실패'
                })
        except Exception as e:
            print(f"  폴백 검사 실패: {e}")
        
        return vulnerabilities
    
    def generate_report(self):
        """종합 취약점 보고서 생성"""
        print("\n" + "="*60)
        print("크롤러 취약점 종합 보고서")
        print("="*60)
        
        # 각 크롤러 분석
        mpb_vulns = self.analyze_mpb_crawler()
        call_vulns = self.analyze_call_rates_crawler()
        interest_vulns = self.analyze_interest_rates_crawler()
        
        # 보고서 생성
        report = {
            'timestamp': datetime.now().isoformat(),
            'mpb_crawler': {
                'vulnerabilities': mpb_vulns,
                'critical_count': len([v for v in mpb_vulns if 'CRITICAL' in v.get('impact', '')])
            },
            'call_rates_crawler': {
                'vulnerabilities': call_vulns,
                'critical_count': len([v for v in call_vulns if 'CRITICAL' in v.get('impact', '')])
            },
            'interest_rates_crawler': {
                'vulnerabilities': interest_vulns,
                'critical_count': len([v for v in interest_vulns if 'CRITICAL' in v.get('impact', '')])
            }
        }
        
        # 콘솔 출력
        print("\n### MPB 크롤러 취약점 ###")
        for vuln in mpb_vulns:
            print(f"- [{vuln.get('impact', 'UNKNOWN')}] {vuln['type']}")
            if 'detail' in vuln:
                print(f"  상세: {vuln['detail']}")
        
        print("\n### 콜금리 크롤러 취약점 ###")
        for vuln in call_vulns:
            print(f"- [{vuln.get('impact', 'UNKNOWN')}] {vuln['type']}")
            if 'detail' in vuln:
                print(f"  상세: {vuln['detail']}")
        
        print("\n### 기준금리 크롤러 취약점 ###")
        for vuln in interest_vulns:
            print(f"- [{vuln.get('impact', 'UNKNOWN')}] {vuln['type']}")
            if 'detail' in vuln:
                print(f"  상세: {vuln['detail']}")
        
        # JSON 파일로 저장
        with open('vulnerability_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print("\n✅ 취약점 보고서가 vulnerability_report.json에 저장되었습니다.")
        
        # 치명적 취약점 요약
        total_critical = sum([
            report['mpb_crawler']['critical_count'],
            report['call_rates_crawler']['critical_count'],
            report['interest_rates_crawler']['critical_count']
        ])
        
        if total_critical > 0:
            print(f"\n⚠️ 경고: {total_critical}개의 치명적 취약점이 발견되었습니다!")
        else:
            print("\n✅ 치명적 취약점은 발견되지 않았습니다.")
        
        return report


if __name__ == "__main__":
    analyzer = CrawlerVulnerabilityAnalyzer()
    analyzer.generate_report()