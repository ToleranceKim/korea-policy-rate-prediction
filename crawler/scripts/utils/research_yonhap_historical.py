import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time

def research_yonhap_historical_access():
    """Research Yonhap's historical article access methods for 10-year data collection"""
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1'
    }
    
    print("ğŸ” ì—°í•©ë‰´ìŠ¤ ê³¼ê±° ê¸°ì‚¬ ì ‘ê·¼ ë°©ë²• ì—°êµ¬")
    print("=" * 50)
    
    # 1. ê¸°ë³¸ ê²€ìƒ‰ URL ë¶„ì„ (ë‹¤ì–‘í•œ ë‚ ì§œ ë²”ìœ„)
    test_dates = [
        ("2025-01-01", "2025-08-18"),  # ìµœê·¼
        ("2024-01-01", "2024-12-31"),  # 1ë…„ ì „
        ("2023-01-01", "2023-12-31"),  # 2ë…„ ì „
        ("2020-01-01", "2020-12-31"),  # 5ë…„ ì „
        ("2015-01-01", "2015-12-31"),  # 10ë…„ ì „
    ]
    
    for start_date, end_date in test_dates:
        print(f"\nğŸ“… í…ŒìŠ¤íŠ¸ ê¸°ê°„: {start_date} ~ {end_date}")
        
        # ë‹¤ì–‘í•œ ê²€ìƒ‰ URL íŒ¨í„´ ì‹œë„
        search_patterns = [
            # íŒ¨í„´ 1: ê¸°ë³¸ ê²€ìƒ‰
            f"https://www.yna.co.kr/search?query=ê¸ˆë¦¬&sort=date&period=range&start={start_date}&end={end_date}",
            # íŒ¨í„´ 2: ê³ ê¸‰ ê²€ìƒ‰
            f"https://www.yna.co.kr/advanced-search?keyword=ê¸ˆë¦¬&from={start_date}&to={end_date}",
            # íŒ¨í„´ 3: API ìŠ¤íƒ€ì¼
            f"https://www.yna.co.kr/api/search?q=ê¸ˆë¦¬&startDate={start_date}&endDate={end_date}",
            # íŒ¨í„´ 4: ì•„ì¹´ì´ë¸Œ
            f"https://www.yna.co.kr/archive/search?keyword=ê¸ˆë¦¬&start={start_date}&end={end_date}",
        ]
        
        for i, url in enumerate(search_patterns, 1):
            try:
                print(f"   íŒ¨í„´ {i}: {url[:80]}...")
                response = requests.get(url, headers=headers, timeout=10)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
                    article_links = soup.find_all('a', href=True)
                    article_count = len([link for link in article_links if '/view/' in link.get('href', '')])
                    
                    print(f"      âœ… ì‘ë‹µ ì„±ê³µ (200) - ê¸°ì‚¬ ë§í¬ {article_count}ê°œ ë°œê²¬")
                    
                    # í˜ì´ì§€ë„¤ì´ì…˜ í™•ì¸
                    pagination = soup.find_all(['div', 'ul'], class_=['pagination', 'paging', 'page'])
                    if pagination:
                        print(f"      ğŸ“„ í˜ì´ì§€ë„¤ì´ì…˜ ë°œê²¬: {len(pagination)}ê°œ")
                    
                elif response.status_code == 404:
                    print(f"      âŒ 404 - URL ì¡´ì¬í•˜ì§€ ì•ŠìŒ")
                elif response.status_code == 403:
                    print(f"      ğŸš« 403 - ì ‘ê·¼ ê¸ˆì§€")
                else:
                    print(f"      âš ï¸ {response.status_code} - ê¸°íƒ€ ì‘ë‹µ")
                    
            except Exception as e:
                print(f"      ğŸ’¥ ì—ëŸ¬: {e}")
            
            time.sleep(1)  # ìš”ì²­ ê°„ê²©
    
    print("\n" + "=" * 50)
    print("ğŸ” ëŒ€ì•ˆ ì ‘ê·¼ ë°©ë²• ì—°êµ¬")
    
    # 2. ì„¹ì…˜ë³„ ì•„ì¹´ì´ë¸Œ ì ‘ê·¼ ì—°êµ¬
    sections = [
        "/economy/finance",
        "/economy/market", 
        "/economy/bank",
        "/politics/economyPolicy"
    ]
    
    for section in sections:
        print(f"\nğŸ“‚ ì„¹ì…˜: {section}")
        
        # ê³¼ê±° ë‚ ì§œë³„ ì ‘ê·¼ ì‹œë„
        test_years = [2025, 2024, 2023, 2020, 2015]
        
        for year in test_years:
            archive_urls = [
                f"https://www.yna.co.kr{section}?year={year}",
                f"https://www.yna.co.kr{section}/{year}",
                f"https://www.yna.co.kr/archive{section}?year={year}",
            ]
            
            for url in archive_urls:
                try:
                    response = requests.get(url, headers=headers, timeout=5)
                    if response.status_code == 200:
                        print(f"   âœ… {year}ë…„ ì ‘ê·¼ ê°€ëŠ¥: {url}")
                        break
                except:
                    continue
            else:
                print(f"   âŒ {year}ë…„ ì ‘ê·¼ ë¶ˆê°€")
            
            time.sleep(0.5)
    
    print("\n" + "=" * 50)
    print("ğŸ” sitemap ë° robots.txt ë¶„ì„")
    
    # 3. sitemap ë¶„ì„
    sitemap_urls = [
        "https://www.yna.co.kr/sitemap.xml",
        "https://www.yna.co.kr/sitemap/news.xml",
        "https://www.yna.co.kr/robots.txt",
    ]
    
    for url in sitemap_urls:
        try:
            response = requests.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                print(f"âœ… {url} ì ‘ê·¼ ê°€ëŠ¥")
                if 'sitemap' in url:
                    # sitemap ë‚´ìš© ë¶„ì„
                    content = response.text
                    if '2015' in content or '2016' in content:
                        print("   ğŸ“… 2015-2016ë…„ ë°ì´í„° í¬í•¨ ê°€ëŠ¥ì„± ìˆìŒ")
                    if '<loc>' in content:
                        urls_count = content.count('<loc>')
                        print(f"   ğŸ“Š ì´ {urls_count}ê°œ URL í¬í•¨")
            else:
                print(f"âŒ {url} ì ‘ê·¼ ë¶ˆê°€ ({response.status_code})")
        except Exception as e:
            print(f"ğŸ’¥ {url} ì—ëŸ¬: {e}")

if __name__ == "__main__":
    research_yonhap_historical_access()