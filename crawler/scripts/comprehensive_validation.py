#!/usr/bin/env python3
"""
í¬ë¡¤ëŸ¬ ëˆ„ë½ ë°©ì§€ë¥¼ ìœ„í•œ í¬ê´„ì  ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ ë°ì´í„°ì™€ í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ ëˆ„ë½ ê²€ì¦
"""

import json
import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime
import time
import subprocess
import os
from pathlib import Path

class ComprehensiveCrawlerValidator:
    """í¬ê´„ì  í¬ë¡¤ëŸ¬ ê²€ì¦ê¸°"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })
        self.validation_results = {}
        
    def validate_mpb_crawler(self):
        """MPB í¬ë¡¤ëŸ¬ ì™„ì „ì„± ê²€ì¦"""
        print("\n" + "="*60)
        print("MPB í¬ë¡¤ëŸ¬ ì™„ì „ì„± ê²€ì¦")
        print("="*60)
        
        # 1. ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê¸°ëŒ€ ë°ì´í„° ìˆ˜ì§‘
        print("\n1ë‹¨ê³„: ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘...")
        expected_data = self.collect_expected_mpb_data()
        
        # 2. í¬ë¡¤ëŸ¬ ì‹¤í–‰
        print("\n2ë‹¨ê³„: í¬ë¡¤ëŸ¬ ì‹¤í–‰...")
        crawled_data = self.run_mpb_crawler()
        
        # 3. ë¹„êµ ë¶„ì„
        print("\n3ë‹¨ê³„: ë°ì´í„° ë¹„êµ ë¶„ì„...")
        validation = self.compare_mpb_data(expected_data, crawled_data)
        
        self.validation_results['mpb'] = validation
        return validation
    
    def collect_expected_mpb_data(self):
        """ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ì—ì„œ 2014-2025ë…„ MPB ë°ì´í„° ìˆ˜ì§‘"""
        expected_items = []
        missing_patterns = []
        
        # ìƒ˜í”Œë§: ì£¼ìš” í˜ì´ì§€ë“¤ë§Œ í™•ì¸ (ì „ì²´ëŠ” ë„ˆë¬´ ë§ìŒ)
        sample_pages = [1, 50, 100, 200, 300, 500, 1000, 1500, 2000, 2500, 3000]
        
        for page in sample_pages:
            try:
                url = f"https://www.bok.or.kr/portal/singl/newsData/listCont.do"
                params = {
                    'targetDepth': 4,
                    'menuNo': 200789,
                    'pageIndex': page
                }
                
                resp = self.session.get(url, params=params)
                soup = BeautifulSoup(resp.text, 'html.parser')
                
                # ëª¨ë“  ê²Œì‹œë¬¼ í™•ì¸
                items = soup.select('li.bbsRowCls')
                
                for item in items:
                    title_elem = item.select_one('a.title')
                    if title_elem:
                        title = title_elem.text.strip()
                        
                        # ì—¬ëŸ¬ ë‚ ì§œ íŒ¨í„´ ì‹œë„
                        year = None
                        
                        # íŒ¨í„´ 1: í‘œì¤€ í˜•ì‹
                        date_match = re.search(r'(\d{4})[\.\së…„\-/](\d{1,2})[\.\sì›”\-/](\d{1,2})', title)
                        if date_match:
                            year = int(date_match.group(1))
                        
                        # íŒ¨í„´ 2: ì—°ë„ë§Œ
                        if not year:
                            year_match = re.search(r'(20\d{2})ë…„?(?:\s|$|\]|\))', title)
                            if year_match:
                                year = int(year_match.group(1))
                        
                        # íŒ¨í„´ 3: ë“±ë¡ì¼ì—ì„œ ì¶”ì¶œ
                        if not year:
                            date_elem = item.select_one('span.date')
                            if date_elem:
                                date_text = date_elem.text
                                year_match = re.search(r'(20\d{2})', date_text)
                                if year_match:
                                    year = int(year_match.group(1))
                        
                        if year and 2014 <= year <= 2025:
                            # ì˜ì‚¬ë¡ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸ (ëŠìŠ¨í•œ ë§¤ì¹­)
                            keywords = ['í†µí™”ì •ì±…', 'ê¸ˆìœµí†µí™”', 'ì˜ì‚¬ë¡', 'íšŒì˜ë¡', 'ìœ„ì›íšŒ', 
                                      'ê¸°ì¤€ê¸ˆë¦¬', 'MPB', 'í†µì•ˆ', 'ì˜ì•ˆ']
                            
                            is_relevant = any(kw in title for kw in keywords)
                            
                            expected_items.append({
                                'title': title,
                                'year': year,
                                'page': page,
                                'relevant': is_relevant
                            })
                        elif not year:
                            missing_patterns.append({
                                'title': title,
                                'page': page,
                                'issue': 'ë‚ ì§œ íŒ¨í„´ ë¯¸ì¸ì‹'
                            })
                
                time.sleep(0.5)  # ì„œë²„ ë¶€ë‹´ ë°©ì§€
                
            except Exception as e:
                print(f"  í˜ì´ì§€ {page} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        print(f"  ìƒ˜í”Œ í˜ì´ì§€ì—ì„œ {len(expected_items)}ê°œ í•­ëª© ë°œê²¬")
        if missing_patterns:
            print(f"  ë‚ ì§œ ì¸ì‹ ì‹¤íŒ¨: {len(missing_patterns)}ê°œ")
            for mp in missing_patterns[:3]:  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
                print(f"    - {mp['title'][:50]}...")
        
        return {
            'items': expected_items,
            'missing_patterns': missing_patterns
        }
    
    def run_mpb_crawler(self):
        """MPB í¬ë¡¤ëŸ¬ ì‹¤í–‰ ë° ê²°ê³¼ ìˆ˜ì§‘"""
        output_file = '/tmp/mpb_validation_test.json'
        
        # ê¸°ì¡´ íŒŒì¼ ì‚­ì œ
        if os.path.exists(output_file):
            os.remove(output_file)
        
        # í¬ë¡¤ëŸ¬ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš© ì œí•œëœ í˜ì´ì§€)
        cmd = [
            '/opt/anaconda3/envs/ds_env/bin/scrapy',
            'crawl', 'mpb_crawler_perfect',
            '-a', 'start_year=2014',
            '-a', 'end_year=2025',
            '-o', output_file,
            '-s', 'CLOSESPIDER_PAGECOUNT=50',  # í…ŒìŠ¤íŠ¸ìš© ì œí•œ
            '-s', 'LOG_LEVEL=ERROR'
        ]
        
        print(f"  í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘... (50í˜ì´ì§€ ì œí•œ)")
        
        try:
            # í¬ë¡¤ëŸ¬ ë””ë ‰í† ë¦¬ë¡œ ì´ë™
            os.chdir('/Users/lord_jubin/Desktop/my_git/mpb-stance-mining/crawler/MPB/mpb_crawler')
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
            
            if result.returncode != 0:
                print(f"  í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì˜¤ë¥˜: {result.stderr}")
                return {'items': [], 'error': result.stderr}
            
            # ê²°ê³¼ íŒŒì¼ ì½ê¸°
            if os.path.exists(output_file):
                with open(output_file, 'r', encoding='utf-8') as f:
                    items = json.load(f)
                    print(f"  í¬ë¡¤ëŸ¬ê°€ {len(items)}ê°œ í•­ëª© ìˆ˜ì§‘")
                    return {'items': items}
            else:
                print("  ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return {'items': []}
                
        except subprocess.TimeoutExpired:
            print("  í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼")
            return {'items': [], 'error': 'timeout'}
        except Exception as e:
            print(f"  í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return {'items': [], 'error': str(e)}
    
    def compare_mpb_data(self, expected, crawled):
        """ê¸°ëŒ€ ë°ì´í„°ì™€ í¬ë¡¤ë§ ë°ì´í„° ë¹„êµ"""
        validation = {
            'expected_count': len(expected.get('items', [])),
            'crawled_count': len(crawled.get('items', [])),
            'missing_items': [],
            'coverage_rate': 0,
            'issues': []
        }
        
        # ì œëª© ê¸°ë°˜ ë§¤ì¹­
        crawled_titles = set()
        for item in crawled.get('items', []):
            if 'title' in item:
                # ì œëª© ì •ê·œí™”
                title = re.sub(r'\s+', ' ', item['title'].strip())
                crawled_titles.add(title)
        
        # ëˆ„ë½ í•­ëª© ì°¾ê¸°
        for exp_item in expected.get('items', []):
            if exp_item.get('relevant', False):
                normalized_title = re.sub(r'\s+', ' ', exp_item['title'].strip())
                if normalized_title not in crawled_titles:
                    validation['missing_items'].append(exp_item)
        
        # Coverage ê³„ì‚°
        relevant_expected = [i for i in expected.get('items', []) if i.get('relevant', False)]
        if relevant_expected:
            validation['coverage_rate'] = (
                (len(relevant_expected) - len(validation['missing_items'])) 
                / len(relevant_expected) * 100
            )
        
        # ì£¼ìš” ì´ìŠˆ ì‹ë³„
        if expected.get('missing_patterns'):
            validation['issues'].append({
                'type': 'ë‚ ì§œ íŒ¨í„´ ë¯¸ì¸ì‹',
                'count': len(expected['missing_patterns']),
                'examples': expected['missing_patterns'][:3]
            })
        
        if crawled.get('error'):
            validation['issues'].append({
                'type': 'í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì˜¤ë¥˜',
                'error': crawled['error']
            })
        
        return validation
    
    def validate_call_rates_crawler(self):
        """ì½œê¸ˆë¦¬ í¬ë¡¤ëŸ¬ ê²€ì¦"""
        print("\n" + "="*60)
        print("ì½œê¸ˆë¦¬ í¬ë¡¤ëŸ¬ ì™„ì „ì„± ê²€ì¦")
        print("="*60)
        
        validation = {
            'date_range_coverage': {},
            'missing_dates': [],
            'data_integrity': []
        }
        
        # ë‚ ì§œ ë²”ìœ„ë³„ ê²€ì¦
        test_ranges = [
            ('20140101', '20140131'),  # 2014ë…„ 1ì›”
            ('20191201', '20191231'),  # 2019ë…„ 12ì›”
            ('20240801', '20240831'),  # 2024ë…„ 8ì›”
        ]
        
        for start, end in test_ranges:
            try:
                url = "https://www.korcham.net/nCham/Service/EconBrief/appl/ProspectBoardList.asp"
                params = {
                    'board_type': 1,
                    'daybt': 'OldNow',
                    'm_OldDate': start,
                    'm_NowDate': end,
                    'pageno': 1
                }
                
                resp = self.session.get(url, params=params)
                soup = BeautifulSoup(resp.text, 'html.parser')
                
                # ë°ì´í„° í–‰ ìˆ˜ í™•ì¸
                data_rows = soup.select('table tr')[1:]  # í—¤ë” ì œì™¸
                
                validation['date_range_coverage'][f"{start}-{end}"] = {
                    'expected_days': 30,  # ëŒ€ëµ
                    'found_records': len(data_rows),
                    'complete': len(data_rows) >= 20  # ì£¼ë§ ì œì™¸ ëŒ€ëµ 20ì¼
                }
                
            except Exception as e:
                validation['date_range_coverage'][f"{start}-{end}"] = {
                    'error': str(e)
                }
        
        self.validation_results['call_rates'] = validation
        return validation
    
    def validate_interest_rates_crawler(self):
        """ê¸°ì¤€ê¸ˆë¦¬ í¬ë¡¤ëŸ¬ ê²€ì¦"""
        print("\n" + "="*60)
        print("ê¸°ì¤€ê¸ˆë¦¬ í¬ë¡¤ëŸ¬ ì™„ì „ì„± ê²€ì¦")
        print("="*60)
        
        validation = {
            'javascript_data_found': False,
            'expected_changes': [],
            'data_format_valid': False
        }
        
        try:
            url = "https://www.bok.or.kr/portal/singl/baseRate/list.do?dataSeCd=01&menuNo=200643"
            resp = self.session.get(url)
            
            # JavaScript ë³€ìˆ˜ í™•ì¸
            if 'chartObj2_s' in resp.text:
                validation['javascript_data_found'] = True
                
                # ë°ì´í„° ì¶”ì¶œ
                pattern = r'chartObj2_s\s*=\s*(\[[\s\S]*?\]);'
                match = re.search(pattern, resp.text)
                
                if match:
                    data_str = match.group(1)
                    # JavaScript ê°ì²´ë¥¼ JSONìœ¼ë¡œ ë³€í™˜
                    data_str = re.sub(r'(\w+):', r'"\1":', data_str)
                    data_str = re.sub(r':([\d.]+)', r':"\1"', data_str)
                    
                    try:
                        import json
                        data = json.loads(data_str)
                        
                        # 2014-2025 ë°ì´í„° í•„í„°ë§
                        for item in data:
                            date = item.get('date', '')
                            if date.startswith(('2014', '2015', '2016', '2017', '2018', 
                                              '2019', '2020', '2021', '2022', '2023', 
                                              '2024', '2025')):
                                validation['expected_changes'].append(item)
                        
                        validation['data_format_valid'] = True
                        
                    except:
                        validation['data_format_valid'] = False
            
            # ì•Œë ¤ì§„ ì£¼ìš” ê¸ˆë¦¬ ë³€ê²½ í™•ì¸
            known_changes = [
                '2014-08-14',  # 2.50% â†’ 2.25%
                '2014-10-15',  # 2.25% â†’ 2.00%
                '2020-03-16',  # 1.25% â†’ 0.75% (ì½”ë¡œë‚˜)
                '2020-05-28',  # 0.75% â†’ 0.50%
                '2021-08-26',  # 0.50% â†’ 0.75%
                '2022-04-14',  # 1.25% â†’ 1.50%
                '2023-01-13',  # 3.25% â†’ 3.50%
            ]
            
            found_dates = [item.get('date') for item in validation['expected_changes']]
            validation['known_changes_found'] = [
                date for date in known_changes if date in found_dates
            ]
            validation['missing_known_changes'] = [
                date for date in known_changes if date not in found_dates
            ]
            
        except Exception as e:
            validation['error'] = str(e)
        
        self.validation_results['interest_rates'] = validation
        return validation
    
    def generate_report(self):
        """ì¢…í•© ê²€ì¦ ë³´ê³ ì„œ ìƒì„±"""
        print("\n" + "="*60)
        print("í¬ë¡¤ëŸ¬ ì™„ì „ì„± ì¢…í•© ë³´ê³ ì„œ")
        print("="*60)
        
        # ê° í¬ë¡¤ëŸ¬ ê²€ì¦
        mpb_result = self.validate_mpb_crawler()
        call_result = self.validate_call_rates_crawler()
        interest_result = self.validate_interest_rates_crawler()
        
        # ë³´ê³ ì„œ ì¶œë ¥
        print("\n### MPB í¬ë¡¤ëŸ¬ ê²€ì¦ ê²°ê³¼ ###")
        print(f"- ì˜ˆìƒ í•­ëª©: {mpb_result.get('expected_count', 0)}ê°œ")
        print(f"- ìˆ˜ì§‘ í•­ëª©: {mpb_result.get('crawled_count', 0)}ê°œ")
        print(f"- Coverage: {mpb_result.get('coverage_rate', 0):.1f}%")
        if mpb_result.get('missing_items'):
            print(f"- ëˆ„ë½ í•­ëª©: {len(mpb_result['missing_items'])}ê°œ")
            for item in mpb_result['missing_items'][:3]:
                print(f"  â€¢ {item['title'][:50]}... (í˜ì´ì§€ {item['page']})")
        
        print("\n### ì½œê¸ˆë¦¬ í¬ë¡¤ëŸ¬ ê²€ì¦ ê²°ê³¼ ###")
        for date_range, coverage in call_result.get('date_range_coverage', {}).items():
            if 'error' not in coverage:
                status = "âœ…" if coverage.get('complete') else "âš ï¸"
                print(f"- {date_range}: {status} {coverage.get('found_records')}ê°œ ë ˆì½”ë“œ")
        
        print("\n### ê¸°ì¤€ê¸ˆë¦¬ í¬ë¡¤ëŸ¬ ê²€ì¦ ê²°ê³¼ ###")
        if interest_result.get('javascript_data_found'):
            print(f"âœ… JavaScript ë°ì´í„° ë°œê²¬")
            print(f"- 2014-2025 ê¸ˆë¦¬ ë³€ê²½: {len(interest_result.get('expected_changes', []))}ê°œ")
            print(f"- ì•Œë ¤ì§„ ë³€ê²½ í™•ì¸: {len(interest_result.get('known_changes_found', []))}/{len(interest_result.get('known_changes_found', []) + interest_result.get('missing_known_changes', []))}")
            if interest_result.get('missing_known_changes'):
                print(f"  âš ï¸ ëˆ„ë½ëœ ì•Œë ¤ì§„ ë³€ê²½: {interest_result['missing_known_changes']}")
        else:
            print("âŒ JavaScript ë°ì´í„° ë¯¸ë°œê²¬")
        
        # JSON ì €ì¥
        report_file = 'comprehensive_validation_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.validation_results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ“„ ìƒì„¸ ë³´ê³ ì„œê°€ {report_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ì¢…í•© í‰ê°€
        print("\n" + "="*60)
        print("ì¢…í•© í‰ê°€")
        print("="*60)
        
        critical_issues = []
        
        if mpb_result.get('coverage_rate', 0) < 90:
            critical_issues.append("MPB í¬ë¡¤ëŸ¬ coverage 90% ë¯¸ë§Œ")
        
        if not interest_result.get('javascript_data_found'):
            critical_issues.append("ê¸°ì¤€ê¸ˆë¦¬ JavaScript ë°ì´í„° ì ‘ê·¼ ì‹¤íŒ¨")
        
        if critical_issues:
            print("âš ï¸ ì£¼ìš” ì´ìŠˆ:")
            for issue in critical_issues:
                print(f"  - {issue}")
        else:
            print("âœ… ëª¨ë“  í¬ë¡¤ëŸ¬ê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.")
        
        return self.validation_results


if __name__ == "__main__":
    validator = ComprehensiveCrawlerValidator()
    validator.generate_report()