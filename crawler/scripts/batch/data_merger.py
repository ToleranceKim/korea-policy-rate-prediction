#!/usr/bin/env python3
"""
ì›”ë³„ ìˆ˜ì§‘ ë°ì´í„° ë³‘í•© ì‹œìŠ¤í…œ
- ì›”ë³„ë¡œ ìˆ˜ì§‘ëœ JSON íŒŒì¼ë“¤ì„ í†µí•©
- ì¤‘ë³µ ì œê±° ë° ë°ì´í„° ê²€ì¦
- ìµœì¢… í†µí•© íŒŒì¼ ìƒì„±
"""

import json
import pandas as pd
from pathlib import Path
import logging
from datetime import datetime
import hashlib

class DataMerger:
    def __init__(self, base_dir="/Users/lord_jubin/Desktop/my_git/mpb-stance-mining/crawler"):
        self.base_dir = Path(base_dir)
        self.monthly_dir = self.base_dir / "data" / "monthly"
        self.merged_dir = self.base_dir / "data" / "merged"
        self.merged_dir.mkdir(exist_ok=True)
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.base_dir / "logs" / "data_merger.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def generate_content_hash(self, item):
        """ì»¨í…ì¸  í•´ì‹œ ìƒì„± (ì¤‘ë³µ ì œê±°ìš©)"""
        # ì œëª©ê³¼ ì»¨í…ì¸ ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•´ì‹œ ìƒì„±
        content_key = f"{item.get('title', '')}{item.get('content', '')}"
        return hashlib.md5(content_key.encode('utf-8')).hexdigest()
    
    def load_monthly_data(self, crawler_name):
        """íŠ¹ì • í¬ë¡¤ëŸ¬ì˜ ì›”ë³„ ë°ì´í„° ë¡œë“œ"""
        monthly_files = list(self.monthly_dir.glob(f"{crawler_name}_*.json"))
        monthly_files.sort()  # ë‚ ì§œìˆœ ì •ë ¬
        
        all_data = []
        duplicate_hashes = set()
        
        self.logger.info(f"ğŸ“‚ Loading {len(monthly_files)} monthly files for {crawler_name}")
        
        for file_path in monthly_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                if isinstance(data, list):
                    # ì¤‘ë³µ ì œê±°
                    for item in data:
                        content_hash = self.generate_content_hash(item)
                        if content_hash not in duplicate_hashes:
                            item['content_hash'] = content_hash
                            item['source_file'] = file_path.name
                            all_data.append(item)
                            duplicate_hashes.add(content_hash)
                        else:
                            self.logger.debug(f"Duplicate found in {file_path.name}: {item.get('title', 'No title')[:50]}...")
                else:
                    # ë‹¨ì¼ ì•„ì´í…œ
                    content_hash = self.generate_content_hash(data)
                    if content_hash not in duplicate_hashes:
                        data['content_hash'] = content_hash
                        data['source_file'] = file_path.name
                        all_data.append(data)
                        duplicate_hashes.add(content_hash)
                        
            except Exception as e:
                self.logger.error(f"Error loading {file_path}: {e}")
        
        self.logger.info(f"âœ… Loaded {len(all_data)} unique items for {crawler_name}")
        return all_data
    
    def validate_data_quality(self, data, crawler_name):
        """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        issues = []
        
        for i, item in enumerate(data):
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            required_fields = ['title', 'content', 'date']
            missing_fields = [field for field in required_fields if not item.get(field)]
            
            if missing_fields:
                issues.append(f"Item {i}: Missing fields {missing_fields}")
            
            # ë‚´ìš© ê¸¸ì´ í™•ì¸
            content_length = len(item.get('content', ''))
            if content_length < 50:
                issues.append(f"Item {i}: Content too short ({content_length} chars)")
            
            # ë‚ ì§œ í˜•ì‹ í™•ì¸
            date_str = item.get('date', '')
            try:
                datetime.strptime(date_str, '%Y-%m-%d')
            except ValueError:
                issues.append(f"Item {i}: Invalid date format '{date_str}'")
        
        if issues:
            self.logger.warning(f"âš ï¸ {crawler_name} data quality issues: {len(issues)} problems found")
            for issue in issues[:10]:  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
                self.logger.warning(f"  {issue}")
            if len(issues) > 10:
                self.logger.warning(f"  ... and {len(issues) - 10} more issues")
        else:
            self.logger.info(f"âœ… {crawler_name} data quality: No issues found")
        
        return issues
    
    def create_summary_statistics(self, data, crawler_name):
        """ë°ì´í„° ìš”ì•½ í†µê³„ ìƒì„±"""
        if not data:
            return {}
        
        df = pd.DataFrame(data)
        
        # ë‚ ì§œë³„ ë¶„í¬
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        date_stats = {
            'earliest_date': df['date'].min().strftime('%Y-%m-%d') if df['date'].min() else None,
            'latest_date': df['date'].max().strftime('%Y-%m-%d') if df['date'].max() else None,
            'total_items': len(df),
            'date_range_months': ((df['date'].max() - df['date'].min()).days / 30.44) if df['date'].min() and df['date'].max() else 0
        }
        
        # ì»¨í…ì¸  í†µê³„
        content_stats = {
            'avg_content_length': df['content'].str.len().mean() if 'content' in df else 0,
            'min_content_length': df['content'].str.len().min() if 'content' in df else 0,
            'max_content_length': df['content'].str.len().max() if 'content' in df else 0
        }
        
        # ê´€ë ¨ì„± ì ìˆ˜ í†µê³„ (ìˆëŠ” ê²½ìš°)
        relevance_stats = {}
        if 'relevance_score' in df.columns:
            relevance_stats = {
                'avg_relevance_score': df['relevance_score'].mean(),
                'min_relevance_score': df['relevance_score'].min(),
                'max_relevance_score': df['relevance_score'].max()
            }
        
        return {
            'crawler_name': crawler_name,
            'date_statistics': date_stats,
            'content_statistics': content_stats,
            'relevance_statistics': relevance_stats,
            'generated_at': datetime.now().isoformat()
        }
    
    def merge_crawler_data(self, crawler_name):
        """íŠ¹ì • í¬ë¡¤ëŸ¬ì˜ ë°ì´í„° ë³‘í•©"""
        self.logger.info(f"ğŸ”„ Starting merge for {crawler_name}")
        
        # ì›”ë³„ ë°ì´í„° ë¡œë“œ
        all_data = self.load_monthly_data(crawler_name)
        
        if not all_data:
            self.logger.warning(f"âš ï¸ No data found for {crawler_name}")
            return False
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        quality_issues = self.validate_data_quality(all_data, crawler_name)
        
        # ë‚ ì§œìˆœ ì •ë ¬
        all_data.sort(key=lambda x: x.get('date', ''))
        
        # í†µê³„ ìƒì„±
        stats = self.create_summary_statistics(all_data, crawler_name)
        
        # í†µí•© íŒŒì¼ ì €ì¥
        merged_file = self.merged_dir / f"{crawler_name}_merged.json"
        with open(merged_file, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        
        # í†µê³„ íŒŒì¼ ì €ì¥
        stats_file = self.merged_dir / f"{crawler_name}_statistics.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        # CSV íŒŒì¼ë„ ìƒì„± (ë¶„ì„ ìš©ì´ì„±ì„ ìœ„í•´)
        try:
            df = pd.DataFrame(all_data)
            csv_file = self.merged_dir / f"{crawler_name}_merged.csv"
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            self.logger.info(f"ğŸ“Š CSV file created: {csv_file}")
        except Exception as e:
            self.logger.warning(f"CSV creation failed for {crawler_name}: {e}")
        
        self.logger.info(f"âœ… {crawler_name} merge completed: {len(all_data)} items â†’ {merged_file}")
        self.logger.info(f"ğŸ“ˆ Statistics: {stats['date_statistics']['earliest_date']} ~ {stats['date_statistics']['latest_date']}")
        
        return True
    
    def merge_all_crawlers(self):
        """ëª¨ë“  í¬ë¡¤ëŸ¬ ë°ì´í„° ë³‘í•©"""
        crawlers = ['yonhap', 'edaily', 'infomax', 'bond', 'interest_rates', 'call_ratings', 'mpb']
        
        self.logger.info(f"ğŸš€ Starting merge for all crawlers: {crawlers}")
        
        success_count = 0
        total_items = 0
        
        for crawler_name in crawlers:
            try:
                success = self.merge_crawler_data(crawler_name)
                if success:
                    success_count += 1
                    
                    # ì•„ì´í…œ ìˆ˜ ì¹´ìš´íŠ¸
                    merged_file = self.merged_dir / f"{crawler_name}_merged.json"
                    if merged_file.exists():
                        with open(merged_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            total_items += len(data)
                            
            except Exception as e:
                self.logger.error(f"Merge failed for {crawler_name}: {e}")
        
        # ì „ì²´ ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
        summary = {
            'merge_completed_at': datetime.now().isoformat(),
            'successful_crawlers': success_count,
            'total_crawlers': len(crawlers),
            'total_items_merged': total_items,
            'output_directory': str(self.merged_dir)
        }
        
        summary_file = self.merged_dir / "merge_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ğŸ‰ Merge completed: {success_count}/{len(crawlers)} crawlers, {total_items:,} total items")
        return summary
    
    def cleanup_monthly_files(self, confirm=False):
        """ì›”ë³„ íŒŒì¼ë“¤ ì •ë¦¬ (ì˜µì…˜)"""
        if not confirm:
            self.logger.info("âš ï¸ Use cleanup_monthly_files(confirm=True) to actually delete monthly files")
            return
        
        monthly_files = list(self.monthly_dir.glob("*.json"))
        for file_path in monthly_files:
            file_path.unlink()
        
        self.logger.info(f"ğŸ—‘ï¸ Cleaned up {len(monthly_files)} monthly files")


if __name__ == "__main__":
    merger = DataMerger()
    
    # ì „ì²´ ë³‘í•© ì‹¤í–‰
    print("ğŸ”„ Starting data merge process...")
    summary = merger.merge_all_crawlers()
    
    print(f"\nğŸ“Š MERGE SUMMARY:")
    print(f"âœ… Successful: {summary['successful_crawlers']}/{summary['total_crawlers']} crawlers")
    print(f"ğŸ“ˆ Total items: {summary['total_items_merged']:,}")
    print(f"ğŸ“ Output directory: {summary['output_directory']}")