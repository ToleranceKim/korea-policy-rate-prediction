#!/usr/bin/env python3
"""
ì›”ë³„ ë¶„í•  ìˆ˜ì§‘ ì‹œìŠ¤í…œ
- 10ë…„ê°„ì˜ ë°ì´í„°ë¥¼ ì›”ë³„ë¡œ ì•ˆì „í•˜ê²Œ ìˆ˜ì§‘
- ì‹¤íŒ¨ ì‹œ ì¬ì‹œì‘ ê¸°ëŠ¥
- ì§„í–‰ìƒí™© ì¶”ì  ë° ë°ì´í„° ì¤‘ë³µ ë°©ì§€
"""

import os
import json
import subprocess
import time
from datetime import datetime, timedelta
from pathlib import Path
import logging

class BatchCollector:
    def __init__(self, base_dir="/Users/lord_jubin/Desktop/my_git/mpb-stance-mining/crawler"):
        self.base_dir = Path(base_dir)
        self.progress_file = self.base_dir / "batch_progress.json"
        self.output_dir = self.base_dir / "data" / "monthly"
        self.output_dir.mkdir(exist_ok=True)
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.base_dir / "logs" / "batch_collector.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # ìˆ˜ì§‘ ëŒ€ìƒ í¬ë¡¤ëŸ¬ ì„¤ì •
        self.crawlers = {
            'yonhap': {
                'path': 'yh/yh_crawler',
                'spider': 'yh_fixed',
                'supports_monthly': True
            },
            'edaily': {
                'path': 'edaily/edaily_crawler', 
                'spider': 'edaily_fixed',
                'supports_monthly': True
            },
            'infomax': {
                'path': 'InfoMax/infomax_crawler',
                'spider': 'infomax_fixed', 
                'supports_monthly': True
            },
            'bond': {
                'path': 'BOND',
                'script': 'bond_crawling.py',
                'supports_monthly': True
            },
            'interest_rates': {
                'path': 'interest_rates/interest_rates_crawler',
                'spider': 'interest_rates',
                'supports_monthly': False  # ì „ì²´ ìˆ˜ì§‘ë§Œ ê°€ëŠ¥
            },
            'call_ratings': {
                'path': 'call_ratings/call_ratings_crawler', 
                'spider': 'call_ratings',
                'supports_monthly': False  # ì „ì²´ ìˆ˜ì§‘ë§Œ ê°€ëŠ¥
            },
            'mpb': {
                'path': 'MPB/mpb_crawler',
                'spider': 'mpb_crawler', 
                'supports_monthly': False  # ì „ì²´ ìˆ˜ì§‘ë§Œ ê°€ëŠ¥
            }
        }
        
        # ì§„í–‰ìƒí™© ë¡œë“œ
        self.progress = self.load_progress()
    
    def load_progress(self):
        """ì§„í–‰ìƒí™© ë¡œë“œ"""
        if self.progress_file.exists():
            with open(self.progress_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {
            'completed_months': {},
            'failed_months': {},
            'last_run': None,
            'total_collected': 0
        }
    
    def save_progress(self):
        """ì§„í–‰ìƒí™© ì €ì¥"""
        self.progress['last_run'] = datetime.now().isoformat()
        with open(self.progress_file, 'w', encoding='utf-8') as f:
            json.dump(self.progress, f, indent=2, ensure_ascii=False)
    
    def generate_monthly_periods(self, start_year=2015, end_year=2025, end_month=8):
        """ì›”ë³„ ìˆ˜ì§‘ ê¸°ê°„ ìƒì„±"""
        periods = []
        for year in range(start_year, end_year + 1):
            start_month = 1
            last_month = 12 if year < end_year else end_month
            
            for month in range(start_month, last_month + 1):
                # ì›” ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ê³„ì‚°
                start_date = datetime(year, month, 1)
                if month == 12:
                    end_date = datetime(year + 1, 1, 1) - timedelta(days=1)
                else:
                    end_date = datetime(year, month + 1, 1) - timedelta(days=1)
                
                period_key = f"{year:04d}-{month:02d}"
                periods.append({
                    'key': period_key,
                    'year': year,
                    'month': month,
                    'start_date': start_date.strftime('%Y-%m-%d'),
                    'end_date': end_date.strftime('%Y-%m-%d')
                })
        
        return periods
    
    def is_month_completed(self, crawler_name, period_key):
        """íŠ¹ì • ì›”ì˜ ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        return (crawler_name in self.progress['completed_months'] and 
                period_key in self.progress['completed_months'][crawler_name])
    
    def mark_month_completed(self, crawler_name, period_key, output_file, item_count):
        """ì›”ë³„ ìˆ˜ì§‘ ì™„ë£Œ í‘œì‹œ"""
        if crawler_name not in self.progress['completed_months']:
            self.progress['completed_months'][crawler_name] = {}
        
        self.progress['completed_months'][crawler_name][period_key] = {
            'completed_at': datetime.now().isoformat(),
            'output_file': str(output_file),
            'item_count': item_count
        }
        
        self.progress['total_collected'] += item_count
        self.save_progress()
    
    def mark_month_failed(self, crawler_name, period_key, error_msg):
        """ì›”ë³„ ìˆ˜ì§‘ ì‹¤íŒ¨ í‘œì‹œ"""
        if crawler_name not in self.progress['failed_months']:
            self.progress['failed_months'][crawler_name] = {}
        
        self.progress['failed_months'][crawler_name][period_key] = {
            'failed_at': datetime.now().isoformat(),
            'error': error_msg
        }
        
        self.save_progress()
    
    def collect_monthly_news(self, crawler_name, config, period):
        """ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ì›”ë³„ ìˆ˜ì§‘"""
        period_key = period['key']
        output_file = self.output_dir / f"{crawler_name}_{period_key}.json"
        
        # ì´ë¯¸ ìˆ˜ì§‘ëœ ê²½ìš° ê±´ë„ˆë›°ê¸°
        if self.is_month_completed(crawler_name, period_key):
            self.logger.info(f"â­ï¸ {crawler_name} {period_key} already completed, skipping...")
            return True
        
        self.logger.info(f"ğŸš€ Starting {crawler_name} collection for {period_key}")
        
        try:
            crawler_dir = self.base_dir / config['path']
            cmd = [
                '/opt/anaconda3/envs/ds_env/bin/scrapy', 'crawl', config['spider'],
                '-o', str(output_file),
                '-s', 'CLOSESPIDER_PAGECOUNT=10',  # í…ŒìŠ¤íŠ¸ìš© ì œí•œ
                '-a', f'start_date={period["start_date"]}',
                '-a', f'end_date={period["end_date"]}'
            ]
            
            result = subprocess.run(
                cmd, 
                cwd=crawler_dir,
                capture_output=True, 
                text=True,
                timeout=1800  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
            )
            
            if result.returncode == 0:
                # ìˆ˜ì§‘ëœ ì•„ì´í…œ ìˆ˜ í™•ì¸
                item_count = self.count_json_items(output_file)
                self.mark_month_completed(crawler_name, period_key, output_file, item_count)
                self.logger.info(f"âœ… {crawler_name} {period_key} completed: {item_count} items")
                return True
            else:
                error_msg = f"Scrapy failed: {result.stderr}"
                self.mark_month_failed(crawler_name, period_key, error_msg)
                self.logger.error(f"âŒ {crawler_name} {period_key} failed: {error_msg}")
                return False
                
        except subprocess.TimeoutExpired:
            error_msg = "Collection timeout (30 minutes)"
            self.mark_month_failed(crawler_name, period_key, error_msg)
            self.logger.error(f"â° {crawler_name} {period_key} timed out")
            return False
        except Exception as e:
            error_msg = str(e)
            self.mark_month_failed(crawler_name, period_key, error_msg)
            self.logger.error(f"ğŸ’¥ {crawler_name} {period_key} error: {error_msg}")
            return False
    
    def collect_full_dataset(self, crawler_name, config):
        """ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ (ì›”ë³„ ë¶„í•  ë¶ˆê°€ëŠ¥í•œ í¬ë¡¤ëŸ¬ìš©)"""
        output_file = self.output_dir / f"{crawler_name}_full.json"
        
        # ì´ë¯¸ ìˆ˜ì§‘ëœ ê²½ìš° ê±´ë„ˆë›°ê¸°
        if self.is_month_completed(crawler_name, 'full'):
            self.logger.info(f"â­ï¸ {crawler_name} full dataset already completed, skipping...")
            return True
        
        self.logger.info(f"ğŸš€ Starting {crawler_name} full collection")
        
        try:
            crawler_dir = self.base_dir / config['path']
            
            if 'spider' in config:
                # Scrapy í¬ë¡¤ëŸ¬
                cmd = [
                    '/opt/anaconda3/envs/ds_env/bin/scrapy', 'crawl', config['spider'],
                    '-o', str(output_file)
                ]
                result = subprocess.run(cmd, cwd=crawler_dir, capture_output=True, text=True, timeout=3600)
            else:
                # Python ìŠ¤í¬ë¦½íŠ¸
                cmd = ['/opt/anaconda3/envs/ds_env/bin/python', config['script']]
                result = subprocess.run(cmd, cwd=crawler_dir, capture_output=True, text=True, timeout=3600)
            
            if result.returncode == 0:
                item_count = self.count_json_items(output_file) if output_file.exists() else 0
                self.mark_month_completed(crawler_name, 'full', output_file, item_count)
                self.logger.info(f"âœ… {crawler_name} full dataset completed: {item_count} items")
                return True
            else:
                error_msg = f"Collection failed: {result.stderr}"
                self.mark_month_failed(crawler_name, 'full', error_msg)
                self.logger.error(f"âŒ {crawler_name} full dataset failed: {error_msg}")
                return False
                
        except Exception as e:
            error_msg = str(e)
            self.mark_month_failed(crawler_name, 'full', error_msg)
            self.logger.error(f"ğŸ’¥ {crawler_name} full dataset error: {error_msg}")
            return False
    
    def count_json_items(self, file_path):
        """JSON íŒŒì¼ì˜ ì•„ì´í…œ ìˆ˜ ì„¸ê¸°"""
        try:
            if not file_path.exists():
                return 0
            
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return len(data) if isinstance(data, list) else 1
        except:
            return 0
    
    def run_monthly_collection(self, crawler_names=None, resume_from=None):
        """ì›”ë³„ ë¶„í•  ìˆ˜ì§‘ ì‹¤í–‰"""
        if crawler_names is None:
            crawler_names = list(self.crawlers.keys())
        
        periods = self.generate_monthly_periods()
        
        # ì¬ì‹œì‘ ì§€ì  ì°¾ê¸°
        start_idx = 0
        if resume_from:
            for i, period in enumerate(periods):
                if period['key'] == resume_from:
                    start_idx = i
                    break
        
        self.logger.info(f"ğŸ“… Starting monthly collection for {len(periods)} periods")
        self.logger.info(f"ğŸ¯ Target crawlers: {crawler_names}")
        
        for crawler_name in crawler_names:
            if crawler_name not in self.crawlers:
                self.logger.warning(f"Unknown crawler: {crawler_name}")
                continue
                
            config = self.crawlers[crawler_name]
            
            if not config['supports_monthly']:
                # ì „ì²´ ìˆ˜ì§‘ë§Œ ê°€ëŠ¥í•œ í¬ë¡¤ëŸ¬
                self.collect_full_dataset(crawler_name, config)
                continue
            
            # ì›”ë³„ ìˆ˜ì§‘ ê°€ëŠ¥í•œ í¬ë¡¤ëŸ¬
            success_count = 0
            total_periods = len(periods[start_idx:])
            
            for period in periods[start_idx:]:
                success = self.collect_monthly_news(crawler_name, config, period)
                if success:
                    success_count += 1
                
                # ìˆ˜ì§‘ ê°„ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                time.sleep(5)
                
                # ì§„í–‰ë¥  ì¶œë ¥
                progress_pct = (success_count / total_periods) * 100
                self.logger.info(f"ğŸ“Š {crawler_name} progress: {success_count}/{total_periods} ({progress_pct:.1f}%)")
            
            self.logger.info(f"ğŸ‰ {crawler_name} monthly collection completed: {success_count}/{total_periods} successful")
    
    def print_status(self):
        """í˜„ì¬ ìˆ˜ì§‘ ìƒíƒœ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ“Š BATCH COLLECTION STATUS")
        print("="*60)
        
        print(f"ğŸ“ˆ Total items collected: {self.progress['total_collected']:,}")
        print(f"ğŸ• Last run: {self.progress.get('last_run', 'Never')}")
        
        print("\nğŸ¯ CRAWLER STATUS:")
        for crawler_name in self.crawlers:
            completed = len(self.progress['completed_months'].get(crawler_name, {}))
            failed = len(self.progress['failed_months'].get(crawler_name, {}))
            print(f"  {crawler_name:15} âœ… {completed:3d} completed  âŒ {failed:2d} failed")
        
        if self.progress['failed_months']:
            print("\nâŒ FAILED COLLECTIONS:")
            for crawler_name, failures in self.progress['failed_months'].items():
                for period, info in failures.items():
                    print(f"  {crawler_name} {period}: {info['error'][:50]}...")


if __name__ == "__main__":
    collector = BatchCollector()
    
    # í˜„ì¬ ìƒíƒœ ì¶œë ¥
    collector.print_status()
    
    # ìˆ˜ì§‘ ì‹œì‘ (í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ë§Œ)
    print("\nğŸš€ Starting test collection...")
    collector.run_monthly_collection(['yonhap', 'edaily', 'infomax'])