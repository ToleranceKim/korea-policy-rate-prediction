#!/usr/bin/env python3
"""
í”„ë¡œì íŠ¸ ì •ë¦¬ í›„ ê²½ë¡œ ì˜¤ë¥˜ ìˆ˜ì • ìŠ¤í¬ë¦½íŠ¸
"""

import re
from pathlib import Path
import logging

class PathFixer:
    def __init__(self, base_dir="/Users/lord_jubin/Desktop/my_git/mpb-stance-mining/crawler"):
        self.base_dir = Path(base_dir)
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def fix_batch_collector(self):
        """BatchCollector ê²½ë¡œ ìˆ˜ì •"""
        file_path = self.base_dir / "scripts" / "batch" / "batch_collector.py"
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ê²½ë¡œ ìˆ˜ì •
        content = content.replace(
            'self.output_dir = self.base_dir / "monthly_output"',
            'self.output_dir = self.base_dir / "data" / "monthly"'
        )
        
        content = content.replace(
            'logging.FileHandler(self.base_dir / "batch_collector.log")',
            'logging.FileHandler(self.base_dir / "logs" / "batch_collector.log")'
        )
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        self.logger.info("âœ… Fixed BatchCollector paths")
    
    def fix_data_merger(self):
        """DataMerger ê²½ë¡œ ìˆ˜ì •"""
        file_path = self.base_dir / "scripts" / "batch" / "data_merger.py"
        
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ê²½ë¡œ ìˆ˜ì •
        content = content.replace(
            'self.monthly_dir = self.base_dir / "monthly_output"',
            'self.monthly_dir = self.base_dir / "data" / "monthly"'
        )
        
        content = content.replace(
            'self.merged_dir = self.base_dir / "merged_output"',
            'self.merged_dir = self.base_dir / "data" / "merged"'
        )
        
        content = content.replace(
            'logging.FileHandler(self.base_dir / "data_merger.log")',
            'logging.FileHandler(self.base_dir / "logs" / "data_merger.log")'
        )
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        self.logger.info("âœ… Fixed DataMerger paths")
    
    def create_new_run_script(self):
        """ìƒˆë¡œìš´ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± (í¬ë¡¤ëŸ¬ ë£¨íŠ¸ì—)"""
        new_script_path = self.base_dir / "run_batch_collection.py"
        
        content = '''#!/usr/bin/env python3
"""
ì›”ë³„ ë¶„í•  ìˆ˜ì§‘ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (ê²½ë¡œ ìˆ˜ì •ë¨)
"""

import sys
import argparse
from pathlib import Path

# scripts/batch ë””ë ‰í„°ë¦¬ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent / "scripts" / "batch"))

from batch_collector import BatchCollector
from data_merger import DataMerger

def main():
    parser = argparse.ArgumentParser(description='ì›”ë³„ ë¶„í•  ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ')
    parser.add_argument('--crawlers', nargs='+', 
                        choices=['yonhap', 'edaily', 'infomax', 'bond', 'interest_rates', 'call_ratings', 'mpb'],
                        default=['yonhap', 'edaily', 'infomax'],
                        help='ìˆ˜ì§‘í•  í¬ë¡¤ëŸ¬ ì„ íƒ')
    parser.add_argument('--test', action='store_true', help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì†ŒëŸ‰ ìˆ˜ì§‘)')
    parser.add_argument('--resume', type=str, help='íŠ¹ì • ì›”ë¶€í„° ì¬ì‹œì‘ (YYYY-MM í˜•ì‹)')
    parser.add_argument('--merge-only', action='store_true', help='ìˆ˜ì§‘ ì—†ì´ ë³‘í•©ë§Œ ì‹¤í–‰')
    parser.add_argument('--status', action='store_true', help='í˜„ì¬ ìƒíƒœë§Œ í™•ì¸')
    
    args = parser.parse_args()
    
    collector = BatchCollector()
    
    if args.status:
        collector.print_status()
        return
    
    if args.merge_only:
        print("ğŸ”„ Starting merge-only process...")
        merger = DataMerger()
        merger.merge_all_crawlers()
        return
    
    print(f"ğŸš€ Starting batch collection...")
    print(f"ğŸ“‹ Target crawlers: {args.crawlers}")
    
    if args.test:
        print("ğŸ§ª TEST MODE: Limited collection for validation")
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œëŠ” ìµœê·¼ 1ê°œì›”ë§Œ ìˆ˜ì§‘
        from datetime import datetime, timedelta
        end_date = datetime.now()
        start_date = end_date - timedelta(days=30)
        
        # í…ŒìŠ¤íŠ¸ìš© ìˆ˜ë™ ìˆ˜ì§‘
        for crawler_name in args.crawlers:
            if crawler_name in ['yonhap', 'edaily', 'infomax']:
                test_period = {
                    'key': 'test',
                    'start_date': start_date.strftime('%Y-%m-%d'),
                    'end_date': end_date.strftime('%Y-%m-%d')
                }
                config = collector.crawlers[crawler_name]
                collector.collect_monthly_news(crawler_name, config, test_period)
    else:
        # ì „ì²´ ì›”ë³„ ìˆ˜ì§‘
        collector.run_monthly_collection(args.crawlers, args.resume)
    
    # ìˆ˜ì§‘ ì™„ë£Œ í›„ ìƒíƒœ ì¶œë ¥
    print("\\n" + "="*60)
    collector.print_status()
    
    # ë³‘í•© ì‹¤í–‰ ì—¬ë¶€ í™•ì¸
    try:
        if input("\\nğŸ”„ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ë³‘í•©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower() == 'y':
            merger = DataMerger()
            merger.merge_all_crawlers()
    except (EOFError, KeyboardInterrupt):
        print("\\nSkipping merge step...")

if __name__ == "__main__":
    main()
'''
        
        with open(new_script_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        # ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
        new_script_path.chmod(0o755)
        
        self.logger.info("âœ… Created new run_batch_collection.py in crawler root")
    
    def verify_imports(self):
        """ì„í¬íŠ¸ ê²½ë¡œ í™•ì¸"""
        script_path = self.base_dir / "run_batch_collection.py"
        
        try:
            # í…ŒìŠ¤íŠ¸ìš© ì„í¬íŠ¸
            import sys
            sys.path.append(str(self.base_dir / "scripts" / "batch"))
            
            from batch_collector import BatchCollector
            from data_merger import DataMerger
            
            # ê°„ë‹¨í•œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í…ŒìŠ¤íŠ¸
            collector = BatchCollector()
            merger = DataMerger()
            
            self.logger.info("âœ… Import verification successful")
            return True
            
        except ImportError as e:
            self.logger.error(f"âŒ Import error: {e}")
            return False
        except Exception as e:
            self.logger.error(f"âŒ Other error: {e}")
            return False
    
    def check_directory_structure(self):
        """ë””ë ‰í„°ë¦¬ êµ¬ì¡° í™•ì¸"""
        required_dirs = [
            self.base_dir / "data" / "monthly",
            self.base_dir / "data" / "merged",
            self.base_dir / "logs",
            self.base_dir / "scripts" / "batch"
        ]
        
        all_exist = True
        for directory in required_dirs:
            if directory.exists():
                self.logger.info(f"âœ… Directory exists: {directory.name}")
            else:
                self.logger.error(f"âŒ Missing directory: {directory}")
                all_exist = False
        
        return all_exist
    
    def run_fixes(self):
        """ëª¨ë“  ê²½ë¡œ ìˆ˜ì • ì‹¤í–‰"""
        self.logger.info("ğŸ”§ Starting path fixes...")
        
        # 1. ë””ë ‰í„°ë¦¬ êµ¬ì¡° í™•ì¸
        if not self.check_directory_structure():
            self.logger.error("Directory structure issues detected!")
            return False
        
        # 2. ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ ìˆ˜ì •
        self.fix_batch_collector()
        self.fix_data_merger()
        
        # 3. ìƒˆë¡œìš´ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
        self.create_new_run_script()
        
        # 4. ì„í¬íŠ¸ í™•ì¸
        if self.verify_imports():
            self.logger.info("ğŸ‰ All path fixes completed successfully!")
            return True
        else:
            self.logger.error("âŒ Path fixes failed verification")
            return False

if __name__ == "__main__":
    fixer = PathFixer()
    success = fixer.run_fixes()
    
    if success:
        print("\\n" + "="*60)
        print("âœ… PATH FIXES COMPLETED")
        print("="*60)
        print("ğŸ“ Updated paths:")
        print("  - monthly data: crawler/data/monthly/")
        print("  - merged data: crawler/data/merged/")
        print("  - logs: crawler/logs/")
        print("  - scripts: crawler/scripts/batch/")
        print("")
        print("ğŸš€ Usage:")
        print("  python run_batch_collection.py --status")
        print("  python run_batch_collection.py --test")
        print("="*60)
    else:
        print("âŒ Path fixes failed. Please check the logs.")