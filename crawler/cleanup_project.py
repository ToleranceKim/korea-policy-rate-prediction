#!/usr/bin/env python3
"""
í”„ë¡œì íŠ¸ ë””ë ‰í„°ë¦¬ ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸
- ë¶ˆí•„ìš”í•œ íŒŒì¼ ì œê±°
- ë””ë ‰í„°ë¦¬ êµ¬ì¡° ì •ë¦¬
- ì•„ì¹´ì´ë¸Œ í´ë”ë¡œ ì´ë™
"""

import os
import shutil
from pathlib import Path
import logging

class ProjectCleaner:
    def __init__(self, base_dir="/Users/lord_jubin/Desktop/my_git/mpb-stance-mining/crawler"):
        self.base_dir = Path(base_dir)
        self.archive_dir = self.base_dir / "archive"
        self.logs_dir = self.base_dir / "logs"
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.base_dir / "cleanup.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def create_directory_structure(self):
        """ê°œì„ ëœ ë””ë ‰í„°ë¦¬ êµ¬ì¡° ìƒì„±"""
        directories = [
            # ì•„ì¹´ì´ë¸Œ ê´€ë ¨
            self.archive_dir / "bond_pdfs",
            self.archive_dir / "bond_csvs", 
            self.archive_dir / "test_outputs",
            
            # ë¡œê·¸ ê´€ë¦¬
            self.logs_dir,
            
            # ë°ì´í„° ê´€ë¦¬
            self.base_dir / "data" / "raw",
            self.base_dir / "data" / "processed",
            self.base_dir / "data" / "monthly",
            self.base_dir / "data" / "merged",
            
            # ìŠ¤í¬ë¦½íŠ¸ ê´€ë¦¬
            self.base_dir / "scripts" / "batch",
            self.base_dir / "scripts" / "utils"
        ]
        
        for directory in directories:
            directory.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"âœ… Created directory: {directory}")
    
    def clean_system_files(self):
        """ì‹œìŠ¤í…œ íŒŒì¼ë“¤ ì œê±°"""
        system_files = list(self.base_dir.rglob(".DS_Store"))
        
        for file_path in system_files:
            try:
                file_path.unlink()
                self.logger.info(f"ğŸ—‘ï¸ Removed system file: {file_path}")
            except Exception as e:
                self.logger.error(f"Failed to remove {file_path}: {e}")
        
        return len(system_files)
    
    def move_bond_files(self):
        """BOND ë””ë ‰í„°ë¦¬ ì •ë¦¬"""
        bond_dir = self.base_dir / "BOND"
        
        if not bond_dir.exists():
            return 0, 0
        
        # PDF íŒŒì¼ë“¤ ì´ë™
        pdf_files = list(bond_dir.glob("*.pdf"))
        pdf_count = 0
        
        for pdf_file in pdf_files:
            try:
                target = self.archive_dir / "bond_pdfs" / pdf_file.name
                shutil.move(str(pdf_file), str(target))
                pdf_count += 1
            except Exception as e:
                self.logger.error(f"Failed to move PDF {pdf_file}: {e}")
        
        # dataset_2 ë””ë ‰í„°ë¦¬ ì´ë™
        dataset_dir = bond_dir / "dataset_2"
        csv_count = 0
        
        if dataset_dir.exists():
            try:
                target = self.archive_dir / "bond_csvs"
                if target.exists():
                    shutil.rmtree(target)
                shutil.move(str(dataset_dir), str(target))
                
                # CSV íŒŒì¼ ê°œìˆ˜ ì„¸ê¸°
                csv_count = len(list((self.archive_dir / "bond_csvs").glob("*.csv")))
                
            except Exception as e:
                self.logger.error(f"Failed to move dataset_2: {e}")
        
        self.logger.info(f"ğŸ“¦ Moved {pdf_count} PDF files and {csv_count} CSV files from BOND")
        return pdf_count, csv_count
    
    def clean_test_files(self):
        """í…ŒìŠ¤íŠ¸ íŒŒì¼ë“¤ ì •ë¦¬"""
        test_patterns = [
            "*_test.json",
            "*_output.json", 
            "*_validation*.json",
            "debug_*.py",
            "test_*.py"
        ]
        
        moved_files = 0
        
        for pattern in test_patterns:
            test_files = list(self.base_dir.rglob(pattern))
            
            for test_file in test_files:
                # ì¤‘ìš”í•œ ì‹œìŠ¤í…œ íŒŒì¼ë“¤ì€ ê±´ë“œë¦¬ì§€ ì•ŠìŒ
                if any(skip in str(test_file) for skip in ['batch_collector', 'data_merger', 'run_batch']):
                    continue
                
                try:
                    target = self.archive_dir / "test_outputs" / test_file.name
                    shutil.move(str(test_file), str(target))
                    moved_files += 1
                    self.logger.info(f"ğŸ“¦ Moved test file: {test_file.name}")
                except Exception as e:
                    self.logger.error(f"Failed to move {test_file}: {e}")
        
        return moved_files
    
    def move_logs(self):
        """ë¡œê·¸ íŒŒì¼ë“¤ ì •ë¦¬"""
        log_files = list(self.base_dir.glob("*.log"))
        moved_logs = 0
        
        for log_file in log_files:
            try:
                target = self.logs_dir / log_file.name
                shutil.move(str(log_file), str(target))
                moved_logs += 1
                self.logger.info(f"ğŸ“ Moved log file: {log_file.name}")
            except Exception as e:
                self.logger.error(f"Failed to move log {log_file}: {e}")
        
        return moved_logs
    
    def reorganize_data_directories(self):
        """ë°ì´í„° ë””ë ‰í„°ë¦¬ ì¬êµ¬ì„±"""
        moves = [
            # monthly_output -> data/monthly
            (self.base_dir / "monthly_output", self.base_dir / "data" / "monthly"),
            # merged_output -> data/merged  
            (self.base_dir / "merged_output", self.base_dir / "data" / "merged")
        ]
        
        for source, target in moves:
            if source.exists():
                try:
                    if target.exists():
                        shutil.rmtree(target)
                    shutil.move(str(source), str(target))
                    self.logger.info(f"ğŸ“ Moved {source.name} to {target}")
                except Exception as e:
                    self.logger.error(f"Failed to move {source}: {e}")
    
    def move_utility_scripts(self):
        """ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸ë“¤ ì •ë¦¬"""
        utility_scripts = [
            "batch_collector.py",
            "data_merger.py", 
            "run_batch_collection.py"
        ]
        
        moved_scripts = 0
        
        for script in utility_scripts:
            script_path = self.base_dir / script
            if script_path.exists():
                try:
                    target = self.base_dir / "scripts" / "batch" / script
                    shutil.move(str(script_path), str(target))
                    moved_scripts += 1
                    self.logger.info(f"ğŸ”§ Moved utility script: {script}")
                except Exception as e:
                    self.logger.error(f"Failed to move {script}: {e}")
        
        # ê¸°íƒ€ ì—°êµ¬ìš© ìŠ¤í¬ë¦½íŠ¸ë“¤
        research_scripts = list(self.base_dir.glob("research_*.py"))
        for script in research_scripts:
            try:
                target = self.base_dir / "scripts" / "utils" / script.name
                shutil.move(str(script), str(target))
                moved_scripts += 1
                self.logger.info(f"ğŸ”¬ Moved research script: {script.name}")
            except Exception as e:
                self.logger.error(f"Failed to move research script {script}: {e}")
        
        return moved_scripts
    
    def create_readme_files(self):
        """ê° ë””ë ‰í„°ë¦¬ì— README íŒŒì¼ ìƒì„±"""
        readme_contents = {
            self.archive_dir: """# Archive Directory

ì´ ë””ë ‰í„°ë¦¬ëŠ” í”„ë¡œì íŠ¸ ì •ë¦¬ ê³¼ì •ì—ì„œ ì´ë™ëœ íŒŒì¼ë“¤ì„ ë³´ê´€í•©ë‹ˆë‹¤.

## êµ¬ì¡°:
- `bond_pdfs/`: ì±„ê¶Œ ê´€ë ¨ PDF íŒŒì¼ë“¤ (ìˆ˜ì§‘ëœ ë¦¬ì„œì¹˜ ë³´ê³ ì„œ)
- `bond_csvs/`: ì²˜ë¦¬ëœ ì±„ê¶Œ ë°ì´í„° CSV íŒŒì¼ë“¤ (6000+ ê°œ)
- `test_outputs/`: ê°œë°œ ì¤‘ ìƒì„±ëœ í…ŒìŠ¤íŠ¸ ì¶œë ¥ íŒŒì¼ë“¤

ì´ íŒŒì¼ë“¤ì€ ì•ˆì „í•˜ê²Œ ë³´ê´€ë˜ë©°, í•„ìš” ì‹œ ì–¸ì œë“  ë³µì› ê°€ëŠ¥í•©ë‹ˆë‹¤.
""",
            
            self.logs_dir: """# Logs Directory

ì‹œìŠ¤í…œ ë¡œê·¸ íŒŒì¼ë“¤ì´ ì €ì¥ë˜ëŠ” ë””ë ‰í„°ë¦¬ì…ë‹ˆë‹¤.

- ë°°ì¹˜ ìˆ˜ì§‘ ë¡œê·¸
- ë°ì´í„° ë³‘í•© ë¡œê·¸  
- í¬ë¡¤ëŸ¬ ì‹¤í–‰ ë¡œê·¸
- ì‹œìŠ¤í…œ ì •ë¦¬ ë¡œê·¸
""",
            
            self.base_dir / "data": """# Data Directory

ìˆ˜ì§‘ ë° ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.

## êµ¬ì¡°:
- `raw/`: ì›ë³¸ ìˆ˜ì§‘ ë°ì´í„°
- `processed/`: ê°€ê³µëœ ë°ì´í„°
- `monthly/`: ì›”ë³„ ë¶„í•  ìˆ˜ì§‘ ë°ì´í„°
- `merged/`: í†µí•©ëœ ìµœì¢… ë°ì´í„°
""",
            
            self.base_dir / "scripts": """# Scripts Directory

ì‹œìŠ¤í…œ ê´€ë¦¬ ë° ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸ë“¤ì„ ë³´ê´€í•©ë‹ˆë‹¤.

## êµ¬ì¡°:
- `batch/`: ë°°ì¹˜ ìˆ˜ì§‘ ì‹œìŠ¤í…œ ìŠ¤í¬ë¦½íŠ¸ë“¤
- `utils/`: ì—°êµ¬ ë° ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸ë“¤
"""
        }
        
        for directory, content in readme_contents.items():
            readme_path = directory / "README.md"
            try:
                with open(readme_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                self.logger.info(f"ğŸ“„ Created README: {readme_path}")
            except Exception as e:
                self.logger.error(f"Failed to create README at {readme_path}: {e}")
    
    def run_cleanup(self, confirm=False):
        """ì „ì²´ ì •ë¦¬ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        if not confirm:
            self.logger.warning("âš ï¸ DRY RUN MODE - No files will be moved")
            self.logger.warning("Use run_cleanup(confirm=True) to actually perform cleanup")
            return
        
        self.logger.info("ğŸ§¹ Starting project cleanup...")
        
        # 1. ë””ë ‰í„°ë¦¬ êµ¬ì¡° ìƒì„±
        self.create_directory_structure()
        
        # 2. ì‹œìŠ¤í…œ íŒŒì¼ ì •ë¦¬
        system_files_count = self.clean_system_files()
        
        # 3. BOND íŒŒì¼ë“¤ ì´ë™
        pdf_count, csv_count = self.move_bond_files()
        
        # 4. í…ŒìŠ¤íŠ¸ íŒŒì¼ë“¤ ì •ë¦¬
        test_files_count = self.clean_test_files()
        
        # 5. ë¡œê·¸ íŒŒì¼ë“¤ ì´ë™
        log_files_count = self.move_logs()
        
        # 6. ë°ì´í„° ë””ë ‰í„°ë¦¬ ì¬êµ¬ì„±
        self.reorganize_data_directories()
        
        # 7. ìŠ¤í¬ë¦½íŠ¸ë“¤ ì •ë¦¬
        script_files_count = self.move_utility_scripts()
        
        # 8. README íŒŒì¼ë“¤ ìƒì„±
        self.create_readme_files()
        
        # ì •ë¦¬ ì™„ë£Œ ë³´ê³ ì„œ
        summary = f"""
ğŸ‰ PROJECT CLEANUP COMPLETED

ğŸ“Š SUMMARY:
- System files removed: {system_files_count}
- PDF files archived: {pdf_count}
- CSV files archived: {csv_count}  
- Test files moved: {test_files_count}
- Log files organized: {log_files_count}
- Scripts reorganized: {script_files_count}

ğŸ“ NEW STRUCTURE:
- archive/: ì•„ì¹´ì´ë¸Œëœ íŒŒì¼ë“¤
- logs/: ë¡œê·¸ íŒŒì¼ë“¤
- data/: ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬ë˜ëŠ” ë°ì´í„°
- scripts/: ì •ë¦¬ëœ ìŠ¤í¬ë¦½íŠ¸ë“¤

âœ… Project is now clean and well-organized!
        """
        
        self.logger.info(summary)
        print(summary)


if __name__ == "__main__":
    cleaner = ProjectCleaner()
    
    # ì •ë¦¬ ì‹¤í–‰ (í™•ì¸ í›„)
    print("ğŸ§¹ Project cleanup ready to run...")
    print("This will reorganize your project structure and move files to appropriate locations.")
    
    if input("Proceed with cleanup? (y/N): ").lower() == 'y':
        cleaner.run_cleanup(confirm=True)
    else:
        print("Cleanup cancelled.")
        cleaner.run_cleanup(confirm=False)  # Dry run to show what would happen